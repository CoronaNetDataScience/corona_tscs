---
title: "A Retrospective Bayesian Model for Measuring Covariate Effects on Observed COVID-19 Test and Case Counts"
<<<<<<< HEAD
author: |
  | Robert Kubinec
  | New York University Abu Dhabi
date: "April 7th, 2020"
=======
author: 
  - Robert Kubinec:
      email: rmk7@nyu.edu
      institute: nyuad
      correspondence: true
  - Luiz Max Carvalho:
      institute: gvf
institute:
  - nyuad: New York University Abu Dhabi
  - gvf: School of Applied Mathematics, Getulio Vargas Foundation
date: "April 10th, 2020"
>>>>>>> 5f16de2666077f7eb0e17d9be1d14f4ee43e1251
toc: false
csl: nature.csl
output: 
  #bookdown::word_document2
  bookdown::pdf_document2:
    keep_tex: true
    includes:
      in_header:
          preamble.tex
<<<<<<< HEAD
bibliography: BibTexDatabase.bib
abstract: "As the COVID-19 outbreak increases, increasing numbers of researchers are examining how an array of factors either hurt or help the spread of the disease. Unfortunately, the majority of available data, primarily confirmed cases of COVID-19, are widely known to be biased indicators of the spread of the disease. In this paper I present a retrospective Bayesian model that is much simpler than epidemiological models of disease progression but is still able to identify the effect of covariates on the historical infection rate. I show that while observed cases and test counts cannot be used to measure the true infection rate due to confounding between the infection rate and the observed data, it is possible to use informative priors derived from susceptible-infected-recovered (SIR) models to estimate associations between background factors and the disease. The model is validated by comparing estimation of the count of infected to projections from expert surveys and extant disease forecasts. To apply the model, I show that as of April 7th, higher infection rates are increasingly concentrated in U.S. states with smaller economies, larger youth populations, less public health funding and fewer Trump voters. On the other hand, the percentage of foreign born residents, air quality, and the number of smokers and cardiovascular deaths are not clear predictors of COVID-19 trends.^[To reproduce the model and to access the underlying Stan code, please see my [Github page](https://github.com/saudiwin/corona_tscs). This paper is part of the [CoronaNet project](https://lumesserschmidt.github.io/CoronaNet/) collecting data on government responses to the COVID-19 pandemic. For helpful comments I thank Cindy Cheng, Joan Barcelo and Luiz Max Cavalho.]"
=======
    pandoc_args:
      - '--lua-filter=scholarly-metadata.lua'
      - '--lua-filter=author-info-blocks.lua'
  #bookdown::word_document2
bibliography: BibTexDatabase.bib
abstract: "As the COVID-19 outbreak progresses, increasing numbers of researchers are examining how an array of factors either hurt or help the spread of the disease. Unfortunately, the majority of available data, primarily confirmed cases of COVID-19, are widely known to be biased indicators of the spread of the disease. In this paper we present a retrospective Bayesian model that is much simpler than epidemiological models of disease progression but is still able to identify the effect of covariates on the historical infection rate. The model is validated by comparing our estimation of the count of infected to projections from expert surveys and extant disease forecasts. To apply the model, we show that as of April 10th, there are approximately 2 million infected people in the United States, and these people are increasingly concentrated in states with less wealth, better air quality, fewer smokers, fewer people under the age of 18, less public health funding and more cardiovascular deaths. On the other hand, the percentage of foreign born residents and the proportion of people who voted for President Trump in 2016 are not clear predictors of COVID-19 trends.^[To reproduce the model and to access the underlying Stan code, please see our [Github page](https://github.com/saudiwin/corona_tscs). This paper is part of the [CoronaNet project](https://lumesserschmidt.github.io/CoronaNet/) collecting data on government responses to the COVID-19 pandemic. For helpful comments we thank Cindy Cheng and Joan Barcelo.]"
>>>>>>> 5f16de2666077f7eb0e17d9be1d14f4ee43e1251
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning=FALSE,message=FALSE)
require(dplyr)
require(tidyr)
require(ggplot2)
require(rstan)
require(stringr)
require(lubridate)
require(bayesplot)
require(historydata)
require(readr)
require(datasets)
require(extraDistr)
require(patchwork)
require(rstanarm) # for observed data modeling
require(tidycensus)

set.seed(662817)

rstan_options(auto_write=T)

knitr::opts_chunk$set(warning=F,message=F)

system2("git",args=c("-C ~/covid-tracking-data","pull"))
system2("git",args=c("-C ~/covid-19-data","pull"))

# whether to run model (it will take a few hours) or load saved model from disk

run_model <- F

pan_model <- stan_model("corona_tscs_betab.stan")

```


\newpage

# Introduction

<<<<<<< HEAD
As more and more data has become available on observed case counts of the SARS-CoV2 coronavirus, there have been increasing attempts to infer how contextual factors like government policies, partisanship, and temperature affect the disease's spread [@carleton2020;@sajadi2020;@dudel2020;@tansim2020]. The temptation to make inferences from the observed data, however, can lead to misleading conclusions. For example, some policy makers have publicly questioned whether the predictions of epidemiological models are far worse than the observed case count.^[See article available at https://www.realclearpolitics.com/video/2020/03/26/dr_birx_coronavirus_data_d] By contrast, in this paper I show that the unobserved infection rate is a confounding variable affecting any estimates of covariates on the observed counts of COVID-19 cases and tests. For this reason, in this paper I present a retrospective Bayesian model that can adjust for this bias by estimating the unseen infection rate up to an unidentified constant. Furthermore, by incorporating informative priors from the susceptible-infected-recovered (SIR)/susceptible-exposed-infected-recovered (SEIR) papers on SARS-CoV2  [@peak2020;@riou2020;@verity2020;@perkins2020;@lourenco2020;@li2020;@ferguson2020], it is possible to put an informative prior on the unseen infection rate and estimate both recent disease trends and the effect of covariates on the historical spread of the disease.


In this paper, I show how the model can be applied by measuring the association between U.S. state-level factors and the disease as of April 7th, 2020, including the timing of state of emergency declarations, vote share in the 2016 election for President Donald Trump, the percentage of foreign born and younger residents, gross domestic product (GDP), and health-related factors. The results show that as of April 7th, higher infection rates are increasingly concentrated in U.S. states with smaller economies, larger youth populations, less public health funding and fewer Trump voters. On the other hand, the percentage of foreign born residents, air quality, and the number of smokers and cardiovascular deaths are not clear predictors of COVID-19 trends.
=======
As more and more data has become available on observed case counts of the SARS-CoV2 coronavirus, there have been increasing attempts to infer how contextual factors like government policies, partisanship, and temperature affect the disease's spread [@carleton2020;@sajadi2020;@dudel2020;@tansim2020;@wright2020;@flaxman2020]. The temptation to make inferences from the observed data, however, can result in misleading conclusions. For example, some policy makers have publicly questioned whether the predictions of epidemiological models are far worse than the observed case count.^[See article available at https://www.realclearpolitics.com/video/2020/03/26/dr_birx_coronavirus_data_d] By contrast, in this paper we show that the unobserved infection rate is a confounding variable affecting any estimates of covariates on the observed counts of COVID-19 cases and tests. For this reason, in this paper we present a retrospective Bayesian model that can adjust for this bias by estimating the unseen infection rate up to an unidentified constant. Furthermore, by incorporating informative priors from the susceptible-infected-recovered (SIR)/susceptible-exposed-infected-recovered (SEIR) papers on SARS-CoV2  [@peak2020;@riou2020;@verity2020;@perkins2020;@lourenco2020;@li2020;@ferguson2020], it is possible to put an informative prior on the unobserved infection rate and estimate both recent disease trends and the effect of covariates on the historical spread of the disease.

We also show how the model can be applied by measuring the association between U.S. state-level factors and the disease as of April 10th, 2020, including the timing of state of emergency declarations, vote share in the 2016 election for President Donald Trump, the percentage of foreign born and younger residents, gross domestic product (GDP) per capita, and health-related factors. The results show that as of April 10th, there are approximately 2 million infected people in the United States, and these people are increasingly concentrated in U.S. states with less wealth, better air quality, fewer smokers, fewer people under the age of 18, less public health funding and more cardiovascular deaths. On the other hand, the percentage of foreign born residents and the proportion of people who voted for President Trump in 2016 are not clear predictors of COVID-19 trends.
>>>>>>> 5f16de2666077f7eb0e17d9be1d14f4ee43e1251

# Methods

In this section I present an intuitive overview of the model, and I refer the interested reader to the supplemental materials for a more complete exposition combined with Monte Carlo simulations showing recovery of the latent infection rate. 

<<<<<<< HEAD
The compartmental model employed by epidemiologists to study disease, and in particular SARS-CoV2 [@peak2020;@riou2020;@verity2020;@perkins2020;@lourenco2020;@li2020;@ferguson2020], estimates different classes of individuals in the population, denoted $S$ for susceptible, $I$ for infected, and $R$ for recovered (other letters may be added such as $E$ for exposed). Using ordinary differential equations, the count of the infected can then be estimated by equating these three identities assuming a fixed population size, as seems reasonable during a relatively quick epidemic. These models guide our understanding of the disease and its progression, and have made warnings about the disease's spread that are proving true on a daily basis.
=======
Compartmental models employed by epidemiologists to study disease, and in particular SARS-CoV2 [@peak2020;@riou2020;@verity2020;@perkins2020;@lourenco2020;@li2020;@ferguson2020], suppose different classes (compartments) of individuals in the population, denoted $S$ for susceptible, $I$ for infectious, and $R$ for removed (other compartments may be added such as $E$ for exposed). The model is usually written in the form of a system of ordinary differential equations (ODEs) and assumes a fixed population size, as seems reasonable during a relatively quick epidemic.
The number infected individuals can then be obtained from the solution of the ODE system for the $I$ compartment.
These models guide our understanding of the disease and its progression, and have made warnings about the disease's spread that are proving true on a daily basis.
>>>>>>> 5f16de2666077f7eb0e17d9be1d14f4ee43e1251

By contrast, this paper endeavors to estimate a much simpler quantity than the entire evolution of the outbreak. Many researchers and the general public often want to learn about what has already happened, or the *empirical* infection rate (also called the attack rate in the epidemiological literature). For a number of time points $t \in T$ since the outbreak's start and countries/regions $c \in C$, I aim to identify the following quantity:

$$
f_t \left (\frac{I_{ct}}{S_{ct}+R_{ct}} \right )
$$

<<<<<<< HEAD
Assuming a fixed population size, this quantity is simply the marginal rate of infections in the population up to the present. Because these are the quantities themselves, not their derivatives, there is no attempt to simultaneously solve for all the parameters. The function $f_t$ determines the historical time trend of the rate of infection (which is assumed to be same across countries/regions) in the population up to time $T$, the present. Because the denominator is shifting over time due to disease progression dynamics, this model is only useful for retrospection, i.e., to examine factors that may be influencing the empirical time trend $f_t$. As $S_{ct}$ and $R_{ct}$ are exogenous to the model, the model cannot predict future prevalence of the disease given that it does not determine these crucial factors. In other words, this model can be seen as a local linear approximation to the $I_{ct}$ curve from an SIR model.
=======
Assuming a fixed population size, this quantity is simply the marginal rate of infections in the population up to the present.
The function $f_t$ determines the historical time trend of the rate of infection (which is assumed to be same across countries/regions) in the population up to time $T$, the present. Because the denominator is shifting over time due to disease progression dynamics, this model is only useful for retrospection, i.e., to examine factors that may be influencing the empirical time trend $f_t$. As $S_{ct}$ and $R_{ct}$ are exogenous to the model, the model cannot predict future prevalence of the disease given that it does not determine these crucial factors. In other words, this model can be seen as a local linear approximation to the $I_{ct}$ curve from an SIR model.
>>>>>>> 5f16de2666077f7eb0e17d9be1d14f4ee43e1251

However, we do not have estimates of the actual infected rate $I_{ct}$, only positive COVID-19 cases $a_{ct}$ and numbers of COVID-19 tests $q_{ct}$. Given this limitation, the aim of the model is to backwards infer the infection rate $I_{ct}$ as a latent process given observed test and counts. Modeling the latent process is necessary to avoid bias in using only observed case counts as a proxy for $I_{ct}$. The reason for this is shown in Figure \ref{tikzfig} in which a covariate $X_{ct}$, such as temperature, is hypothesized to affect the infection rate $I_{ct}$. Unfortunately, increasing infection rates can cause both increasing numbers of observed counts $a_{ct}$ and tests $q_{ct}$. As more people are infected, more tests are likely to be done, which will increase the number of cases independently of the infection rate. As a result, due to the back-door path from the infection rate $I_{ct}$ to case counts $a_{ct}$ via the number of tests $q_{ct}$, it is impossible to infer the effect of $X_{ct}$ on $I_{ct}$ from the observed data alone without modeling the latent infection rate. 

\begin{figure}
\label{tikzfig}
\caption{Directed Acyclic Graph Showing Confounding of Covariate $X_{ct}$ on Observed Tests $q_{ct}$ and Cases $a_{ct}$ Due to Unobserved Infection Rate $I_{ct}$}
\ctikzfig{policy_dag}
\footnotesize{Figure shows the relationship between a covariate $X_{ct}$ representing a policy or social factor influencing the infection rate $I_{ct}$. Because the infection rate $I_{ct}$ influences both the number of reported tests $q_{ct}$ and reported cases $a_{ct}$, any regression of a covariate $X_{ct}$ on the reported data will be biased.}
\end{figure}

The Bayesian model presented in the supplementary materials provides a full explanation of how to model the infection rate's influence on both cases and tests simultaneously. I further show in the supplementary materials that two restrictions are necessary to identify the sign and rank of the effect of covariates $X_{ct}$ on $I_{ct}$: the paths $I_{ct} \rightarrow q_{ct}$ and $I_{ct} \rightarrow a_{ct}$ must be strictly positive so that an increasing infection rate will have a non-decreasing effect on cases and tests. Given these restrictions and weakly informative priors on the parameters, it is possible to know whether $X_{ct}$ is associated with increasing or decreasing $I_{ct}$, though not with respect to the actual number of infected people, only in terms of the covariate's sign or rank relative to other covariates.

Furthermore, I show in the supplementary materials that if further information can be put on the ratio between the true number of infected individuals $I_{ct}$ and the number of tests $q_{ct}$, it is possible to transform inferences from the model to approximate counts of infected people up to the present. Thankfully, this information is available through epidemiological models of the disease, which offer inferences on the number of un-diagnosed cases [@li2020;@peak2020]. Based on these estimates, I can put an informative prior in the model that the number of tests is likely to be at least 10% of the total number of infected individuals.   With this prior information from disease simulations, it is possible to come up with an empirical estimate of infected rates and covariate effects that is more useful to the general public than solely observed tests and cases, as I demonstrate in the next section.

<<<<<<< HEAD
## Data Availability

All data analyzed in this article are available in a Github repository: https://github.com/saudiwin/corona_tscs.
=======
## Data & Code Availability
>>>>>>> 5f16de2666077f7eb0e17d9be1d14f4ee43e1251

All data analyzed in this article and code used to run models are available in a Github repository: https://github.com/saudiwin/corona_tscs.


# Results

The only data required to fit the model, in addition to the covariates of interest, are observed cases and tests for COVID-19 by day. In this section, I fit the model to numbers of COVID-19 case counts on US states and territories provided by [The New York Times](https://github.com/nytimes/covid-19-data). By doing so, we can use the differences in trajectories across states to help identify the effect of state-level covariates on the infection rate. I supplement these observed case counts with testing data by day from the [COVID-19 Tracking Project](https://github.com/COVID19Tracking/covid-tracking-data). The testing data starts at March 4th, so I impute the testing data back in time by assuming that the average case/tests ratio stays the same to the origin of the outbreak. Furthermore, as there are discrepancies where the reported number of tests in some states like New Jersey is less than the total number of cases, I impute the number of tests via the case/test ratio for the sample as a whole.

<<<<<<< HEAD
To analyze the effect of suppression policies, I proxy for preparedness to fight the epidemic by including the date that states of emergencies were declared across U.S. states and territories in the model.^[See this site for dates and relevant sources: https://en.wikipedia.org/wiki/U.S._state_and_local_government_response_to_the_2020_coronavirus_pandemic] The suppression covariate is then equal to a vector of days which is mean centered and standardized. I further add in state-level data on Donald Trump's vote share for the 2016 election from the MIT Election Lab, a 2019 estimate of state GDP from the Bureau of Economic Analysis, the 2018 percentage of foreign born residents from the U.S. Census Bureau, and 2019 state-level average data on air pollution, cardiovascular deaths per capita, percentage of residents under age 18, number of dedicated health care providers, public health funding, and smoking rates provided by the United Health Foundation.

In this section I first fit a partially-identified model without any information about the true number of infected people to demonstrate that it is possible to obtain estimates of covariates, though with substantial uncertainty. I then show how we can use insight from SIR/SEIR models to add in informative prior information on the likely number of infected people and translate the estimates into probable infection rates. I then show that these estimates in fact closely track the predictions of SIR/SEIR models for the U.S. population from March 2020, providing external validity for the method and for these predictions.
=======
To analyze the effect of suppression policies, we proxy for preparedness to fight the epidemic by including the date that states of emergencies were declared across U.S. states and territories in the model.^[See this site for dates and relevant sources: https://en.wikipedia.org/wiki/U.S._state_and_local_government_response_to_the_2020_coronavirus_pandemic] The suppression covariate is then equal to a vector of days. We further add in state-level data on Donald Trump's vote share for the 2016 election from the MIT Election Lab, a 2019 estimate of state GDP from the Bureau of Economic Analysis, the 2018 percentage of foreign born residents from the U.S. Census Bureau, and 2019 state-level average data on air pollution,^[Defined as average exposure of the general public to particulate matter of 2.5 microns or less (PM$_{2.5}$) measured in micrograms per cubic meter (3-year estimate).] cardiovascular deaths per capita, percentage of residents under age 18, number of dedicated health care providers, public health funding, and smoking rates provided by the United Health Foundation [@unhf2019]. All variables are mean-centered and standardized.

In this section we first fit a partially-identified model without any information about the true number of infected people to demonstrate that it is possible to obtain an estimate of the infection trend, though with substantial uncertainty. We then show how we can use insight from SIR/SEIR models to add in informative prior information on the likely number of infected people and translate the estimates into probable infection rates. We then show that these estimates in fact closely track the predictions of SIR/SEIR models for the U.S. population from March 2020, providing external validity for the method and for these predictions.
>>>>>>> 5f16de2666077f7eb0e17d9be1d14f4ee43e1251



```{r munge_data,include=F}

# vote share
# MIT Election Lab
load("../data/mit_1976-2016-president.rdata")

vote_share <- filter(x,candidate=="Trump, Donald J.",
                     party=="republican",
                     writein=="FALSE") %>% 
  mutate(trump=candidatevotes/totalvotes)

# state GDP

state_gdp <- readxl::read_xlsx("../data/qgdpstate0120_0_bea.xlsx",sheet="Table 3") %>% 
  mutate(gdp=Q1 + Q2 + Q3 +Q4)

# US Census data - population & percent foreign-born
# note: you need a Census API key loaded to use this -- see package tidycensus docs

acs_data <- get_acs("state",variables=c("B01003_001","B05002_013"),year=2018,survey="acs1") %>% 
  select(-moe) %>% 
  mutate(variable=recode(variable,
                         B01003_001="state_pop",
                         B05002_013="foreign_born")) %>% 
  spread("variable","estimate") %>% 
  mutate(prop_foreign=foreign_born/state_pop)


# health data

health <- read_csv("../data/2019-Annual.csv") %>% 
  filter(`Measure Name` %in% c("Air Pollution","Cardiovascular Deaths","Dedicated Health Care Provider",
                              "Population under 18 years", "Public Health Funding","Smoking")) %>% 
  select(`Measure Name`,state="State Name",Value) %>% 
  distinct %>% 
  spread(key="Measure Name",value="Value")

merge_names <- tibble(state.abb,
                      state=state.name)

nyt_data <- read_csv("~/covid-19-data/us-states.csv") %>% 
  complete(date,state,fill=list(cases=0,deaths=0,fips=0)) %>% 
  mutate(month_day=ymd(date)) %>% 
  group_by(state) %>% 
    arrange(state,date) %>% 
  mutate(Difference=cases - dplyr::lag(cases),
         Difference=coalesce(Difference,0,0)) %>% 
  left_join(merge_names,by="state")

tests <- read_csv("~/covid-tracking-data/data/states_daily_4pm_et.csv") %>% 
  mutate(month_day=ymd(date)) %>% 
  arrange(state,month_day) %>% 
  group_by(state) %>% 
  mutate(tests_diff=total-dplyr::lag(total),
         cases_diff=positive-dplyr::lag(positive),
         cases_diff=coalesce(cases_diff,positive),
         cases_diff=ifelse(cases_diff<0,0,cases_diff),
         tests_diff=coalesce(tests_diff,total),
         tests_diff=ifelse(tests_diff<0,0,tests_diff)) %>% 
  select(month_day,tests="tests_diff",total,state.abb="state")

# merge cases and tests

combined <- left_join(nyt_data,tests,by=c("state.abb","month_day")) %>% 
  left_join(acs_data,by=c("state"="NAME")) %>% 
  filter(!is.na(state_pop))

# add suppression data

emergency <- read_csv("../data/state_emergency_wikipedia.csv") %>% 
  mutate(day_emergency=dmy(paste0(`State of emergency declared`,"-2020")),
         mean_day=mean(as.numeric(day_emergency),na.rm=T),
         sd_day=sd(as.numeric(day_emergency),na.rm=T),
         day_emergency=((as.numeric(day_emergency) - mean_day)/sd_day)) %>% 
  select(state="State/territory",day_emergency,mean_day,sd_day) %>% 
  mutate(state=substr(state,2,nchar(state))) %>% 
  filter(!is.na(day_emergency))

combined <- left_join(combined,emergency,by="state")

# add in other datasets 

combined <- left_join(combined,health,by="state")
combined <- left_join(combined,select(state_gdp,state,gdp),by="state")
combined <- left_join(combined,select(vote_share,state,trump))

# impute data

combined <- group_by(combined,state) %>% 
  mutate(test_case_ratio=sum(tests,na.rm=T)/sum(Difference,na.rm=T)) %>% 
  ungroup %>% 
  mutate(test_case_ratio=ifelse(test_case_ratio<1 | is.na(test_case_ratio),
                                mean(test_case_ratio[test_case_ratio>1],na.rm=T),test_case_ratio)) %>% 
  group_by(state) %>% 
    mutate(tests=case_when(Difference>0 & is.na(tests)~Difference*test_case_ratio,
                    Difference==0~0,
                    Difference>tests~Difference*test_case_ratio,
                    TRUE~tests),
           gdp=gdp/state_pop) %>% 
  arrange(state) %>% 
  filter(state!="Puerto Rico")

combined <- group_by(combined,state) %>% 
  arrange(month_day) %>% 
  mutate(outbreak=as.numeric(cases>1)) %>% 
  fill(outbreak,.direction="down") %>% 
  mutate(outbreak_time=cumsum(outbreak)) %>% 
  ungroup %>%
  mutate_at(c("Cardiovascular Deaths",
              "outbreak_time",
              "Air Pollution",
              "Dedicated Health Care Provider",
              "Smoking",
              "Population under 18 years",
              "Public Health Funding",
              "gdp",
              "trump",
              "day_emergency",
              "prop_foreign"), ~as.numeric(scale(.))) %>% 
  group_by(month_day) %>% 
  mutate(world_infect=sum(outbreak_time>1))

# create case dataset

cases_matrix <- select(combined,Difference,month_day,state) %>% 
  group_by(month_day,state) %>% 
  summarize(Difference=as.integer(mean(Difference))) %>% 
  spread(key = "month_day",value="Difference")

cases_matrix_num <- as.matrix(select(cases_matrix,-state))

# create tests dataset

tests_matrix <- select(combined,tests,month_day,state) %>% 
  group_by(month_day,state) %>% 
  summarize(tests=as.integer(mean(tests))) %>% 
  spread(key = "month_day",value="tests")

tests_matrix_num <- as.matrix(select(tests_matrix,-state))

# need the outbreak matrix

outbreak_matrix <- as.matrix(lapply(1:ncol(cases_matrix_num), function(c) {
  if(c==1) {
    outbreak <- as.numeric(cases_matrix_num[,c]>0)
  } else {
    outbreak <- as.numeric(apply(cases_matrix_num[,1:c],1,function(col) any(col>0)))
  }
  tibble(outbreak)
}) %>% bind_cols)

colnames(outbreak_matrix) <- colnames(cases_matrix_num)

time_outbreak_matrix <- t(apply(outbreak_matrix,1,cumsum))

just_data <- distinct(select(ungroup(combined),state,day_emergency,state_pop,trump,air="Air Pollution",
                      heart="Cardiovascular Deaths",
                      providers="Dedicated Health Care Provider",
                      young="Population under 18 years",
                      smoking="Smoking",
                      gdp,
                      public_health="Public Health Funding",
                      prop_foreign)) %>% arrange(state)

covs <- select(ungroup(just_data),-state,-state_pop) %>% as.matrix

# now give to Stan

ortho_time <- poly(scale(1:ncol(cases_matrix_num)),degree=3)

time_outbreak_center <- matrix(scale(c(time_outbreak_matrix)),nrow=nrow(time_outbreak_matrix),
                                                   ncol=ncol(time_outbreak_matrix))

real_data <- list(time_all=ncol(cases_matrix_num),
                 num_country=nrow(cases_matrix_num),
                 S=ncol(covs),
                 country_pop=floor(just_data$state_pop/100),
                 cases=cases_matrix_num,
                 ortho_time=ortho_time,
                 phi_scale=.01,
                 count_outbreak=as.numeric(scale(apply(outbreak_matrix,2,sum))),
                 tests=tests_matrix_num,
                 time_outbreak=time_outbreak_matrix,
                 time_outbreak_center=time_outbreak_center,
                 suppress=covs)

init_vals <- function() {
  list(phi_raw=c(30,10),
       world_infect=0.5,
       finding=0.5,
       alpha=c(0,-10))
}



if(run_model) {
  us_fit <- sampling(pan_model,data=real_data,chains=3,cores=3,iter=1500,warmup=1000,
                   init=init_vals)
  
  saveRDS(us_fit,"../data/us_fit.rds")
} else {
  us_fit <- readRDS("../data/us_fit.rds")
}


```

```{r infectstate,fig.cap="5% to 95% HPD Uncertainty Intervals of Partially-Identified Infection Rates by U.S. State with Total Average"}
all_est_state <- as.data.frame(us_fit,"num_infected_high") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="variable",value="estimate",-iter) %>% 
  group_by(variable) %>% 
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")),
         time_point=as.numeric(str_extract(variable,"[1-9][0-9]?0?(?=\\])")),
         time_point=ymd(min(combined$month_day)) + days(time_point-1))

all_est_state <- left_join(all_est_state,tibble(state_num=1:nrow(cases_matrix),
                                                state=cases_matrix$state,
                                                state_pop=real_data$country_pop,
                                    suppress_measures=real_data$suppress,by="state_num"))

# merge in total case count

case_count <- gather(cases_matrix,key="time_point",value="cases",-state) %>% 
  mutate(time_point=ymd(time_point)) %>% 
  group_by(state) %>% 
  arrange(state,time_point) %>% 
  mutate(cum_sum_cases=cumsum(cases)) 

us_case_count <- group_by(case_count,time_point) %>% 
  summarize(all_cum_sum=sum(cum_sum_cases))

all_est_state <- left_join(all_est_state,us_case_count,by="time_point")


all_est_state %>% 
  mutate(estimate=estimate) %>% 
  group_by(state_num,time_point,suppress_measures) %>% 
    summarize(med_est=quantile(estimate,.5),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>% 
  ggplot(aes(y=med_est,x=time_point)) +
  #geom_line(aes(group=state_num,colour=suppress_measures),alpha=0.5) +
  geom_ribbon(aes(ymin=low_est,
  ymax=high_est,
  group=state_num,
  fill=suppress_measures),alpha=0.5) +
  stat_smooth(colour="black") +
  theme_minimal() +
  scale_color_distiller(palette="RdBu",direction=-1) +
  ylab("Latent Infection Scale") +
  labs(caption="5% - 95% HPD Intervals are colored by\nwhen a state declared a state of emergency (standardized count of days).\nAs the total number of infected people is unknown,\nthis chart measures the relative marginal infection rates between states.") +
  xlab("Days Since Outbreak Start") +
  geom_hline(yintercept = 0,linetype=3) +
  guides(fill=guide_colorbar(title="Timing of State Emergency Declaration")) +
  theme(panel.grid = element_blank(),
        legend.position = "bottom")
ggsave("uncertain_state_rates.png")

```

```{r rankcountries1}

rank1 <- all_est_state %>% 
  group_by(state,iter) %>% 
  summarize(estimate=mean(estimate)) %>% 
  ungroup %>% 
  group_by(iter) %>% 
  mutate(rank_state=51 - rank(estimate),
         model="Partially Identified") %>% 
  group_by(state,model) %>% 
    summarize(med_est=mean(rank_state),
            high_est=quantile(rank_state,.95),
            low_est=quantile(rank_state,.05))
  

```

<<<<<<< HEAD
Figure \@ref(fig:infectstate) shows the 5% - 95% high posterior density (HPD) intervals of the latent infection rate by state since January 1st. The intervals are shaded by the relative time when a state declared a state of emergency, which reveals that state of emergency declarations are correlated with higher infection rates. As can be seen, there is a sharp discontinuity in the plot around March 1st when infection rates began to increase. While it would appear that the rate of increase has leveled off in the last week, that is not a supported inference as the scale is the logit scale, so it is similar to the log scale in that higher numbers are farther away than they appear visually. Because the latent scale is not identified, the figure is only showing how the infection rates have evolved from zero to the true but unknown top infection rate. As such, it appears to be slowing as it reaches the top of the scale, but that is simply an illusion of logarithmic growth. 
=======
Figure \@ref(fig:infectstate) shows the 5% - 95% high posterior density (HPD) intervals of the latent infection rate by state since January 1st. The intervals are shaded by the relative time when a state declared a state of emergency, which reveals that state of emergency declarations are correlated with higher infection rates. As can be seen, there is a sharp discontinuity in the plot around March 1st when infection rates began to increase. While it would appear that the rate of increase has leveled off in the last week, that is not a supported inference as the scale is the logit scale, which is similar to the log scale in that higher numbers are farther away than they appear visually. Because the latent scale is not identified, the figure is only showing how the infection rates have evolved from zero to the true but unknown top infection rate. As such, it appears to be slowing as it reaches the top of the scale, but that is simply an illusion of the underlying sigmoid function.
>>>>>>> 5f16de2666077f7eb0e17d9be1d14f4ee43e1251


```{r suppress2,echo=F,include=F}

suppress_effect <- as.data.frame(us_fit,"suppress_effect") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="parameter",value="estimate",-iter) %>% 
  mutate(supp_type=as.numeric(str_extract(parameter,"(?<=\\[)[1-9][0-9]?0?")),
         variable=as.numeric(str_extract(parameter,"[1-9]?0?(?=\\])")))

num_infected <- as.data.frame(us_fit,"num_infected_high") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="variable",value="estimate",-iter) %>% 
  group_by(variable) %>% 
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")),
         time_point=as.numeric(str_extract(variable,"[1-9][0-9]?0?(?=\\])")),
         time_point=ymd(min(combined$month_day)) + days(time_point-1))

# iterate this bugger

p_infected <- group_by(num_infected,iter) %>% 
  summarize(est=mean(dlogis(estimate)))

if(run_model) {
    over_all <- parallel::mclapply(sample(1:max(num_infected$iter),100), function(q) {
    over_supp <- lapply(1:max(suppress_effect$variable), function(s) {
      over_time_level <- lapply(seq(min(time_outbreak_center),max(time_outbreak_center),length.out=100), function(i) {
        this_eff <- p_infected$est[q] * (suppress_effect$estimate[suppress_effect$supp_type==1 & suppress_effect$variable==s & suppress_effect$iter==q] + suppress_effect$estimate[suppress_effect$supp_type==2 & suppress_effect$variable==s & suppress_effect$iter==q]*i)
      
      tibble(estimate=this_eff,
             time_scale_point=i,
             iter=q,
             variable=s)
      }) %>% bind_rows
      
      over_time_level
      
    }) %>% bind_rows
    
    return(over_supp)
  },mc.cores=3) %>% bind_rows
    
    saveRDS(over_all,"../data/over_all.rds")
    
} else {
  over_all <- readRDS("../data/over_all.rds")
}




over_all_time <- group_by(over_all,variable,time_scale_point) %>% 
  summarize(med_est=median(estimate),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>% 
  mutate(model="Partially\nIdentified")


over_all_sum <- group_by(over_all,variable) %>% 
  summarize(med_est=median(estimate),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>% 
  mutate(model="Partially\nIdentified")
  
  

# calculate marginal effects

over_all_time <- left_join(over_all_time,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>% 
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      day_emergency="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

over_all_sum <- left_join(over_all_sum,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>% 
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      day_emergency="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

p1 <- over_all_sum %>%
  ggplot(aes(y=med_est,x=reorder(label,med_est))) +
  geom_pointrange(aes(ymin=low_est,ymax=high_est),alpha=0.8) +
  theme_minimal() +
  xlab("")  +
  ylab("Effect on Proportion Infected") +
  geom_hline(yintercept=0,linetype=2) +
  ggtitle("Constant Effect on\nProportion Infected") +
  theme(panel.grid=element_blank(),
        plot.title = element_text(size=10,hjust=0.5)) +
  coord_flip()

p2 <- over_all_time %>%
 ggplot(aes(y=med_est,x=time_scale_point)) +
    xlab("Time Since Outbreak Began")  +
  ylab("Effect on Proportion Infected") +
  geom_ribbon(aes(ymin=low_est,ymax=high_est),alpha=0.5,fill="red") +
  geom_line(linetype=2) +
  ggtitle("Time-Varying Effect on\nProportion Infected") +
  theme_minimal() +
  geom_hline(yintercept=0,linetype=3) +
  theme(panel.grid=element_blank(),
        plot.title = element_text(size=10,hjust=0.5)) +
  facet_wrap(~label,scales="free_y")

p1 + p2

ggsave("marginal_noid.png")

```

```{r run_over_id_model_scale,include=F}


if(run_model) {
  pan_model_scale <- stan_model("corona_tscs_betab_scale.stan")
  
  us_fit_scale <- sampling(pan_model_scale,data=real_data,chains=3,cores=3,iter=1500,warmup=1000,
                           control=list(adapt_delta=0.95),
                   init=init_vals)
  
  saveRDS(us_fit_scale,"../data/us_fit_scale.rds")
} else {
  us_fit_scale <- readRDS("../data/us_fit_scale.rds")
}


```


```{r naivemodel,include=F}

# calculate the same info but only with observed data

if(run_model) {
  obs_model <- stan_glm(cbind(combined$Difference,floor(combined$state_pop/100) - combined$Difference) ~ poly(outbreak_time,3)[,2:3] +
                     day_emergency*poly(outbreak_time,1)[,1] +
                        `Cardiovascular Deaths`*poly(outbreak_time,1)[,1] +
                        `Air Pollution`*poly(outbreak_time,1)[,1] +
                        `Dedicated Health Care Provider`*poly(outbreak_time,1)[,1] +
                        `Smoking`*poly(outbreak_time,1)[,1] +
                        `Population under 18 years`*poly(outbreak_time,1)[,1] +
                     prop_foreign*poly(outbreak_time,1)[,1] +
                        `Public Health Funding`*poly(outbreak_time,1)[,1] +
                        gdp*poly(outbreak_time,1)[,1] +
                        trump*poly(outbreak_time,1)[,1] +
                     world_infect,
                      data=combined,
                 QR=TRUE,
                      family="binomial",
                 chains=1,cores=1)

saveRDS(obs_model,"../data/obs_model.rds")
} else {
  obs_model <- readRDS("../data/obs_model.rds")
}



out_data <- mcmc_intervals_data(obs_model,regex_pars=":") %>% 
  filter(!grepl(x=parameter,pattern="I"))

```



```{r infectscaled,fig.cap="Approximate Total Number of COVID-19 Infected Individuals in the U.S. as of April 7th",fig.height=5}
all_est_state <- as.data.frame(us_fit_scale,"num_infected_high") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="variable",value="estimate",-iter) %>% 
  group_by(variable) %>% 
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")),
         time_point=as.numeric(str_extract(variable,"[1-9][0-9]?0?(?=\\])")),
         time_point=ymd(min(combined$month_day)) + days(time_point-1))

all_est_state <- left_join(all_est_state,tibble(state_num=1:nrow(cases_matrix),
                                                state=cases_matrix$state,
                                                state_pop=real_data$country_pop,
                                    suppress_measures=real_data$suppress,by="state_num"))

# merge in total case count

case_count <- gather(cases_matrix,key="time_point",value="cases",-state) %>% 
  mutate(time_point=ymd(time_point)) %>% 
  group_by(state) %>% 
  arrange(state,time_point) %>% 
  mutate(cum_sum_cases=cumsum(cases)) 

us_case_count <- group_by(case_count,time_point) %>% 
  summarize(all_cum_sum=sum(cum_sum_cases))

all_est_state <- left_join(all_est_state,us_case_count,by="time_point")

calc_sum <- all_est_state %>% 
  ungroup %>% 
  mutate(estimate=(plogis(estimate)/100)*(state_pop*100)) %>% 
  group_by(state_num,iter) %>% 
  arrange(state_num,time_point) %>% 
  mutate(cum_est=cumsum(estimate)) %>% 
  group_by(time_point,iter,all_cum_sum) %>% 
  summarize(us_total=sum(cum_est)) %>% 
  group_by(time_point,all_cum_sum) %>% 
  summarize(med_est=quantile(us_total,.5),
            high_est=quantile(us_total,.95),
            low_est=quantile(us_total,.05)) 

max_est <- as.integer(round(calc_sum$med_est[calc_sum$time_point==max(calc_sum$time_point)]))
high_max_est <- as.integer(round(calc_sum$high_est[calc_sum$time_point==max(calc_sum$time_point)]))
low_max_est <- as.integer(round(calc_sum$low_est[calc_sum$time_point==max(calc_sum$time_point)]))
max_obs <- calc_sum$all_cum_sum[calc_sum$time_point==max(calc_sum$time_point)]

options(scipen=999)

# load expert survey results

expert_survey <- read_csv("../data/consensusForecastsDB.csv") %>% 
  filter(questionLabel %in% c("QF5","QF4","QF3"),
         surveyIssued>ymd("2020-03-16")) %>% 
    mutate(keep=case_when(surveyIssued==ymd("2020-03-02")~"QF4",
                        surveyIssued==ymd("2020-03-09")~"QF6",
                        surveyIssued==ymd("2020-03-16")~"QF4",
                        surveyIssued==ymd("2020-03-23")~"QF4",
                        surveyIssued==ymd("2020-03-30")~"QF3",
                        TRUE~"reject")) %>% 
  filter(questionLabel==keep,cumprob>0.05,cumprob<0.95) %>% 
  group_by(surveyIssued,questionLabel) %>% 
  summarize(med_est=bin[abs(cumprob-0.5)==min(abs(cumprob-0.5))],
            low_est=bin[abs(cumprob-0.1)==min(abs(cumprob-.1))],
            high_est=bin[abs(cumprob-0.9)==min(abs(cumprob-.9))]) %>% 
  rename(time_point="surveyIssued")


# need to add in cumulative case counts

calc_sum %>% 
  ggplot(aes(y=med_est,x=time_point)) +
  geom_ribbon(aes(ymin=low_est,
  ymax=high_est),
  fill="blue",
  alpha=0.5) +
  geom_line(aes(y=all_cum_sum)) +
  theme_minimal() +
  ylab("Total Number Infected/Reported") +
  scale_y_continuous(labels=scales::comma) +
  labs(caption="Blue 5% - 95% HPD intervals show estimated infected and the black line\nshows observed cases from the New York Times.\nThese estimates are based on the assumption that as few as 10% of cases\nmay be reported based on SIR/SEIR models.") +
  annotate("text",x=ymd(c("2020-04-07","2020-04-07")),
           y=c(max_est,max_obs),
           hjust=1,
           vjust=0,
           fontface="bold",
           size=3,
           label=c(paste0("Estimated Infected:\n",formatC(low_max_est,big.mark=",",format = "f",digits=0)," - ",
                                                         formatC(high_max_est,big.mark=",",format = "f",digits=0)),
                   paste0("Total Reported Cases:\n",formatC(max_obs,big.mark=",")))) +
  annotate("text",x=ymd(c("2020-03-01","2020-03-12",
                          as.character(expert_survey$time_point))),
           y=c(300000,180000,expert_survey$high_est*1.01),
           vjust=0,
           fontface="bold",
           size=2,
           label=c("Li et al. March 8th",
                   "Perkins et al. March 26th",
                   paste0("Expert Survey\n",expert_survey$time_point)),alpha=0.8) +
  # previously published annotations
  annotate("pointrange",x=ymd("2020-03-01"),y=9001,ymin=2299,ymax=20403,alpha=0.5) +
  annotate("pointrange",x=ymd("2020-03-12"),y=22876,ymin=7451,ymax=53044,alpha=0.5) +
  geom_pointrange(data=expert_survey,aes(ymin=low_est,ymax=high_est),alpha=0.5) +
  xlab("Days Since Outbreak Start") +
  theme(panel.grid = element_blank(),
        legend.position = "top")

ggsave("est_vs_obs_experts.png")

```

<<<<<<< HEAD
By comparison Figures \@ref(fig:infectscaled) and \@ref(fig:stateplot) show fully-identified models incorporating informative prior information suggesting that the ratio of tests to infected ratio individuals is probably no less than 10% of those infected (though it could very high). This information, as previously mentioned, was derived from simulation and statistical modeling of COVID-19 outbreaks so far suggesting that a large proportion of infected individuals are undetected [@li2020;@peak2020]. Based on this information, the scale of the latent infection process shown in Figure \@ref(fig:infectstate) can be further identified. Figure \@ref(fig:infectscaled) shows that the likely cumulative number of infected cases, including those who may have recovered or died, is likely approaching 1 million infected individuals in the United States, in line with SIR/SEIR and expert survey projections released recently.^[See https://www.nytimes.com/2020/04/01/world/coronavirus-news.html?action=click&module=Spotlight&pgtype=Homepage for a recent overview.]  Furthermore, Figure \@ref(fig:stateplot) shows significant state by state heterogeneity, with New York showing the greatest number of infected, followed by California. There is some suggestive evidence that California's infected count growth may be slowing, though the large uncertainty interval suggests that this inference would be unwise without more data or assumptions.
=======
By comparison, Figures \@ref(fig:infectscaled) and \@ref(fig:stateplot) show fully-identified models incorporating informative prior information suggesting that the ratio of tests to infected ratio individuals is probably no less than 10% of those infected (though it could very high). This information, as previously mentioned, was derived from simulation and statistical modeling of COVID-19 outbreaks so far suggesting that a large proportion of infected individuals are undetected [@li2020;@peak2020]. Based on this information, the scale of the latent infection process shown in Figure \@ref(fig:infectstate) can be further identified. Figure \@ref(fig:infectscaled) shows that the likely cumulative number of infected cases, including those who may have recovered or died, is likely approaching 2 million infected individuals in the United States, in line with SIR/SEIR and expert survey projections released recently.^[See https://www.nytimes.com/2020/04/01/world/coronavirus-news.html?action=click&module=Spotlight&pgtype=Homepage for a recent overview.]  Furthermore, Figure \@ref(fig:stateplot) shows significant state by state heterogeneity, with New York showing the greatest number of infected, followed by California. There is some suggestive evidence that California's infected count growth may be slowing, though the large uncertainty interval suggests that this inference would be unwise without more data or assumptions.
>>>>>>> 5f16de2666077f7eb0e17d9be1d14f4ee43e1251


```{r stateplot,fig.cap="Average Cumulative Count of Infected People by U.S. State as of April 10th",fig.height=5,echo=F}
require(ggrepel)

calc_sum_state <- all_est_state %>% 
  ungroup %>% 
  mutate(estimate=(plogis(estimate)/100)*(state_pop*100)) %>% 
  group_by(state,iter) %>% 
  arrange(state,time_point) %>% 
  mutate(cum_est=cumsum(estimate)) %>% 
  group_by(time_point,state,suppress_measures) %>% 
  summarize(med_est=quantile(cum_est,.5),
            high_est=quantile(cum_est,.95),
            low_est=quantile(cum_est,.05)) 

# Annotations

# get top 5 plus random 5 

top_5 <- filter(calc_sum_state,time_point==max(calc_sum_state$time_point)) %>% 
  arrange(desc(med_est)) %>% 
  ungroup %>% 
  slice(c(1:5,sample(6:length(unique(calc_sum_state$state)),5))) %>% 
  distinct %>% 
  mutate(label=paste0(state,":",formatC(low_est,big.mark=",",format = "f",digits=0)," - ",
                                                         formatC(high_est,big.mark=",",format = "f",digits=0)))

calc_sum_state %>% 
  ggplot(aes(y=med_est,x=time_point)) +
  geom_line(aes(group=state,colour=med_est)) +
  # geom_ribbon(aes(ymin=low_est,
  # ymax=high_est,
  # group=state_num,
  # fill=suppress_measures),alpha=0.5) +
  theme_minimal() +
  scale_color_distiller(palette="Reds",direction=1) +
  ylab("Cumulative Count") +
  labs(caption="Some lines are labeled with uncertainty of estimates (5% - 95% Interval).\nThese estimates are based on the assumption that as few as\n10% of cases may be reported based on SIR/SEIR models. Does not exclude people\nwho may have recovered or died.") +
  geom_text_repel(data=top_5,aes(x=time_point,y=med_est,label=label),
                  size=2,fontface="bold",segment.colour = NA) +
  scale_y_continuous(labels=scales::comma) +
  xlab("Days Since Outbreak Start") + 
  guides(colour="none") +
  theme(panel.grid = element_blank(),
        legend.position = "top")
ggsave("certain_state_rates.png")
```


```{r rankcountries2,fig.cap="Comparison of Identified and Partially-Identified Models' Ranking of Infected by State",fig.height=4}

# rank2 <- all_est_state %>% 
#   group_by(state,iter) %>% 
#   summarize(estimate=mean(estimate)) %>% 
#   ungroup %>% 
#   group_by(iter) %>% 
#   mutate(rank_state=51 - rank(estimate),
#          model="Identified") %>% 
#   group_by(state,model) %>% 
#     summarize(med_est2=mean(rank_state))
# 
# bind_cols(rank1,rank2) %>% 
#   ggplot(aes(y=med_est,x=med_est2)) +
#   geom_point(alpha=0.8,colour="blue") + 
#   theme_minimal() +
#   xlab("")  +
#   scale_colour_brewer(type="qual") +
#   xlab("Partially Identified State Rank\nof Average Proportion Infected") +
#   ylab("Fully Identified State Rank\nof Average Proportion Infected") +
#   geom_smooth(method="lm") +
#   geom_abline(yintercept=0,slope=1,linetype=2) +
#   theme(panel.grid=element_blank(),
#         plot.title = element_text(size=10,hjust=0.5)) +
#   coord_flip()
  

```

<<<<<<< HEAD
Figure \@ref(fig:compmarg) shows the marginal effect of covariates in the model on the latent infection rate. Each covariate has two kinds of effects, a cumulative effect on the left-hand side and a time-varying effect on the right-hand side, where time is coded as a linear counter since the start of the outbreak (at least one case recorded) in each state. As can be seen, the cumulative effects are generally less precise, but the over-time effects show clear trends. States with larger youth populations, and more health care providers and seeing increasingly higher infection rates. By contrast, states that declared emergencies later, have more Trump voters, smaller economies and less public health funding are seeing increasingly fewer infections. These set of associations are likely not causal, but more an indication of the spatial spread of the disease thus far, with outbreaks starting in the liberal and wealthy coasts. 

However, it is nonetheless important to note that state-level Trump vote share is not associated with increasing COVID-19 infections, despite public opinion polling showing that Americans who are more conservative tend to discount the danger posed by the virus.^[See https://www.vox.com/2020/3/15/21180506/coronavirus-poll-democrats-republicans-trump.] In addition, those states that declared an earlier state of emergency have not yet witnessed a slowing infection rate. Again, as the infection increases, there will be more data to make inter-state comparisons and obtain more information about whether these associations will hold up for the course of the disease. At present, however, there is no reason to believe that states that declared emergencies later or have more Trump voters are facing increasing disease trends at present.

=======
Figure \@ref(fig:compmarg) shows the marginal effect of covariates in the model on the latent infection rate, expressed as the marginal increase in proportions for a standard deviation increase in the covariate. Each covariate has two kinds of marginal effects: a cumulative effect on the left-hand side and a time-varying effect on the right-hand side, where time is coded as a linear counter since the start of the outbreak (at least one case recorded) in each state. As can be seen, the cumulative effects are generally less precise, but the over-time effects show clear trends. States with larger youth populations and more health care providers are seeing increasingly higher infection rates. By contrast, states that have lower GDP per capita, less public health funding, fewer smokers and better air quality are seeing increasingly fewer infections. These set of associations are not necessarily causal, as they are influenced by the spatial spread of the disease thus far, with outbreaks starting in wealthy coastal states and progressively moving inland.

However, it is important to note that state-level Trump vote share is not associated with increasing COVID-19 infections, despite public opinion polling showing that Americans who are more conservative tend to discount the danger posed by the virus.^[See https://www.vox.com/2020/3/15/21180506/coronavirus-poll-democrats-republicans-trump.] In addition, those states that declared an earlier state of emergency have not yet witnessed a slowing infection rate. It is important to notice, however, that there is a roughly 14 day lag between action and effect. Anything states do, be it beneficial or detrimental, and however big is its effect, will take about a fortnight to manifest itself in the data. At present, however, there is no reason to believe that states that declared emergencies later or have more Trump voters are facing increasing disease trends.

>>>>>>> 5f16de2666077f7eb0e17d9be1d14f4ee43e1251
```{r suppress1,echo=F,fig.height=7,fig.cap="Marginal Effects of Covariates on Latent Infection Rates for U.S. States"}

suppress_effect <- as.data.frame(us_fit_scale,"suppress_effect") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="parameter",value="estimate",-iter) %>% 
  mutate(supp_type=as.numeric(str_extract(parameter,"(?<=\\[)[1-9][0-9]?0?")),
         variable=as.numeric(str_extract(parameter,"[1-9]?0?(?=\\])")))

num_infected <- as.data.frame(us_fit_scale,"num_infected_high") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="variable",value="estimate",-iter) %>% 
  group_by(variable) %>% 
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")),
         time_point=as.numeric(str_extract(variable,"[1-9][0-9]?0?(?=\\])")),
         time_point=ymd(min(combined$month_day)) + days(time_point-1))

# iterate this bugger


p_infected <- group_by(num_infected,iter) %>% 
  summarize(est=mean(dlogis(estimate)))

if(run_model) {
    over_all2 <- parallel::mclapply(sample(1:max(num_infected$iter),100), function(q) {
    over_supp <- lapply(1:max(suppress_effect$variable), function(s) {
      over_time_level <- lapply(seq(min(time_outbreak_center),max(time_outbreak_center),length.out=100), function(i) {
        this_eff <- p_infected$est[q] * (suppress_effect$estimate[suppress_effect$supp_type==1 & suppress_effect$variable==s & suppress_effect$iter==q] + suppress_effect$estimate[suppress_effect$supp_type==2 & suppress_effect$variable==s & suppress_effect$iter==q]*i)
      
      tibble(estimate=this_eff,
             time_scale_point=i,
             iter=q,
             variable=s)
      }) %>% bind_rows
      
      over_time_level
      
    }) %>% bind_rows
    
    return(over_supp)
  },mc.cores=3) %>% bind_rows
    
    saveRDS(over_all2,"../data/over_all2.rds")
    
} else {
  over_all2 <- readRDS("../data/over_all2.rds")
}




over_all_time2 <- group_by(over_all2,variable,time_scale_point) %>% 
  summarize(med_est=median(estimate),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>% 
  mutate(model="Fully\nIdentified")


over_all_sum2 <- group_by(over_all2,variable) %>% 
  summarize(med_est=median(estimate),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>% 
  mutate(model="Fully\nIdentified")
  
  

# calculate marginal effects

over_all_time2 <- left_join(over_all_time2,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>% 
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      day_emergency="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

over_all_sum2 <- left_join(over_all_sum2,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>% 
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      day_emergency="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

p3 <- over_all_sum2 %>%
  ggplot(aes(y=med_est,x=reorder(label,med_est))) +
  geom_pointrange(aes(ymin=low_est,ymax=high_est),alpha=0.8) +
  theme_minimal() +
  xlab("")  +
  ylab("Effect on Proportion Infected") +
  geom_hline(yintercept=0,linetype=2) +
  ggtitle("Cumulative Effect on\nProportion Infected") +
  theme(panel.grid=element_blank(),
        plot.title = element_text(size=10,hjust=0.5),
        strip.text=element_text(face="bold")) +
  coord_flip()

p4 <- over_all_time2 %>%
 ggplot(aes(y=med_est,x=time_scale_point)) +
    xlab("Time Since Outbreak Began")  +
  ylab("Effect on Proportion Infected") +
  labs("Intervals are 5% - 95% high posterior density (HPD) uncertainty intervals.") +
  geom_ribbon(aes(ymin=low_est,ymax=high_est),alpha=0.5,fill="red") +
  geom_line(linetype=2) +
  ggtitle("Time-varying Effect on\nProportion Infected") +
  theme_minimal() +
  geom_hline(yintercept=0,linetype=3) +
  theme(panel.grid=element_blank(),
        plot.title = element_text(size=10,hjust=0.5),
        strip.text=element_text(face="bold")) +
  facet_wrap(~label,scales="free_y")

p3 + p4 +
  plot_layout(nrow=2) +
  plot_annotation(caption="Marginal effects calculated as a 1-standard deviation change in a covariate on the\nlatent infection rate. 5% - 95% high posterior density intervals derived from\n100 Markov Chain Monte Carlo posterior draws.")

ggsave("marginal_id.png")

```

<!-- Figure \@ref(fig:rankcountries2) compares the identified and partially-identified models' infection rates by plotting the average posterior rank of infection for each state and each model. As can be seen, while there is stochastic noise, there is a very clear and strong relationship in terms of the ranks, with a linear fit very close to the 45-degree horizontal line indicating a 1:1 relationship. While the models' predictions differ, they are estimating the same latent quantity, albeit with noise. -->

<<<<<<< HEAD
Finally, \@ref(fig:compmarg), compares the underlying parameter estimates from the latent Bayesian model and a binomial model of the observed case counts with the same covariates as predictors (including time trends). As can be seen, the predictions of these models wildly diverge, with the observed case count model showing far higher associations between background factors and case counts. As I discuss in the supplementary material, these larger and overly precise effects are due to ignoring the latent infectious process that is generating the case counts. Of particular worry is when covariates are correlated with a state's ability or willingness to test for the virus. This may explain why the observed model in Figure \@ref(fig:compmarg) shows such a high effect of smoking on reducing infection counts.

Figure \@ref(fig:comptests) shows that smoking rates by state are correlated with testing per capita, with states with larger infections--Washington and New York--also testing more per capita and having relatively fewer smokers than other states like West Virginia. As such, the observed data model is likely obfuscating the correlation between smoking and numbers of tests with the spread of the disease.
=======
Finally, Figure \@ref(fig:compmarg) compares the underlying parameter estimates from the latent Bayesian model and a binomial model of the observed case counts with the same covariates as predictors (including time trends). As can be seen, the estimates of these models can wildly diverge, with the observed case count model showing far larger and implausibly precise associations between covariates and case counts. Of particular worry is when covariates are correlated with a state's ability or willingness to test for the virus.^[We note that testing may be conducted for the virus using PCR, or for the disease, which would also include serology. This also may explain why the observed model in Figure \@ref(fig:compmarg) shows such a high effect of GDP per capita on reducing infection counts.]


For example, the coefficient for per capita GDP shows an implausibly large positive association (+25 on the *logit* scale) in the observed cases model, suggesting that the more wealth in a state, the lower the infection rate. To show the potential confounding in this result, Figure \@ref(fig:comptests) plots GDP per capita against state COVID-19 tests per capita, revealing states with more infected people--Washington and New York--have implemented many tests and are also more wealthy on average. As such, the observed data model is likely obfuscating the strength of the correlation between GDP per capita and the number of tests with the spread of the disease.
>>>>>>> 5f16de2666077f7eb0e17d9be1d14f4ee43e1251


```{r compmarg,fig.cap="Comparison of Effects from Latent and Observed Data Models"}

plot_data1 <- mcmc_intervals_data(us_fit,regex_pars ="effect") %>% 
  mutate(supp_type=as.numeric(str_extract(parameter,"(?<=\\[)[1-9][0-9]?0?")),
         variable=as.numeric(str_extract(parameter,"[1-9]?0?(?=\\])")))

# join back to labels 

plot_data1 <- left_join(plot_data1,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>% 
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      day_emergency="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

plot_data2 <- mcmc_intervals_data(us_fit_scale,regex_pars ="effect") %>% 
  mutate(supp_type=as.numeric(str_extract(parameter,"(?<=\\[)[1-9][0-9]?0?")),
         variable=as.numeric(str_extract(parameter,"[1-9]?0?(?=\\])")))

# join back to labels 

plot_data2 <- left_join(plot_data2,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>% 
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      day_emergency="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

plot_data3 <- mcmc_intervals_data(obs_model,pars=c("day_emergency",
                                                   "trump",
                                                   "`Air Pollution`",
                                                   "`Cardiovascular Deaths`",
                                                   "`Dedicated Health Care Provider`",
                                                   "`Population under 18 years`",
                                                   "Smoking",
                                                   "gdp",
                                                   "prop_foreign",
                                                   "`Public Health Funding`",
                                                   "day_emergency:poly(outbreak_time, 1)[, 1]",
                                                   "poly(outbreak_time, 1)[, 1]:trump",
                                                   "poly(outbreak_time, 1)[, 1]:`Air Pollution`",
                                                   "poly(outbreak_time, 1)[, 1]:`Cardiovascular Deaths`",
                                                   "poly(outbreak_time, 1)[, 1]:`Dedicated Health Care Provider`",
                                                   "poly(outbreak_time, 1)[, 1]:`Population under 18 years`",
                                                   "poly(outbreak_time, 1)[, 1]:Smoking",
                                                   "poly(outbreak_time, 1)[, 1]:gdp",
                                                   "poly(outbreak_time, 1)[, 1]:prop_foreign",
                                                   "poly(outbreak_time, 1)[, 1]:`Public Health Funding`")) %>% 
  mutate(supp_type=rep(c(1,2),each=10),
         variable=rep(1:10,2))

# join back to labels 

plot_data3 <- left_join(plot_data3,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>% 
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      day_emergency="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

plot_data <- bind_rows(list(`Latent\nInfection Rate`=plot_data2,
                            `Observed\nCases`=plot_data3),
                       .id="model")

p1 <- plot_data %>% 
  filter(supp_type==1) %>% 
  ggplot(aes(y=m,x=reorder(label,m))) +
  geom_pointrange(aes(ymin=ll,ymax=hh,colour=model),alpha=0.8,position=position_dodge(width=0.5),size=.5) +
  theme_minimal() +
  xlab("")  +
  ylab("Parameter Estimate\n(Logit Scale)") +
  geom_hline(yintercept=0,linetype=2) +
  scale_color_brewer(type="qual") +
  ggtitle("Constant Coefficients") +
  theme(panel.grid=element_blank(),
        plot.title = element_text(size=10,hjust=0.5),
        legend.position = "top") +
    guides(color=guide_legend(title="")) +
  coord_flip()

p2 <- plot_data %>% 
  filter(supp_type==2) %>% 
  ggplot(aes(y=m,x=reorder(label,m))) +
    xlab("")  +
  ylab("Parameter Estimate\n(Logit Scale)") +
  geom_pointrange(aes(ymin=ll,ymax=hh,colour=model),alpha=0.8,position=position_dodge(width=0.5),size=.5) +
  ggtitle("Time-Varying Coefficients") +
  theme_minimal() +
    scale_color_brewer(type="qual") +
    geom_hline(yintercept=0,linetype=2) +
  theme(panel.grid=element_blank(),
        plot.title = element_text(size=10,hjust=0.5),
        legend.position = "top") +
  guides(color=guide_legend(title="")) +
  coord_flip()

p1 + p2 + plot_annotation(tag_levels="A",
                          caption="Both models were fit with the same covariates and specification, and\nusing the same  Markov Chain Monte Carlo samplers. Parameter values are on the logit scale.\nIntervals are 5% - 95% high posterior density intervals.")

ggsave("effect_compare.png")

```

```{r comptests,fig.cap="Comparison of GDP Per Capita and COVID-19 Tests per 10,000 Residents by State"}

check_tests <- group_by(combined,state,gdp) %>% summarize(mean_tests=(mean(tests,na.rm=T)/state_pop[1])*10000) %>% 
  filter(state!="District of Columbia")

check_tests %>% 
  ggplot(aes(y=gdp,x=mean_tests)) +
    stat_smooth(method="lm",alpha=0.1) +
  geom_text_repel(aes(label=state),alpha=0.8,segment.size=0) +
  theme_minimal() +
  labs("Plot shows some states like New York and Massachusetts have high average income and also\nvery high testing rates.") +
  theme(panel.grid = element_blank()) +
  ylab("GDP Per Capita (Standard Deviation)") +
  xlab("Tests per 10,000 state residents")




```


# Conclusion

<<<<<<< HEAD
This model was written to permit the identification of suppression measures targeted at the spread of COVID-19. It is not intended to be a replacement or alternative to the disease forecasting literature, especially as this model relies on SIR/SEIR estimates for full identification. If anything, this modeling exercise shows why SIR/SEIR models are so important: without them it is literally impossible to know the total number of infected people on a given day. This model's simplicity and ability to use empirical data are its main features, and the hope is that it can be used and extended by researchers looking at government policies and other tertiary factors on the spread of the disease. At the very least, the model provides realistic uncertainty intervals taking into account very real biases in the observed data.
=======
This model was devised to permit the identification of suppression measures and social, political and economic factors on the spread of COVID-19. It is not intended to be a replacement or alternative to the disease forecasting literature, especially as this model relies on SIR/SEIR estimates for full identification. If anything, this modeling exercise shows why explicit mechanistic epidemiological models are so important: without them it is literally impossible to know the total number of infected people on a given day. This model's simplicity and ability to use empirical data are its main features, and the hope is that it can be used and extended by researchers looking at government policies and other tertiary factors on the spread of the disease. At the very least, the model provides realistic uncertainty intervals taking into account very real biases in the observed data.
>>>>>>> 5f16de2666077f7eb0e17d9be1d14f4ee43e1251

To fit the model, it is necessary to have at least an estimate of how many tests have been conducted. The  [CoronaNet](https://lumesserschmidt.github.io/CoronaNet/) project is currently working to obtain testing data, in addition to information about government policy responses to COVID-19, in an effort to better understand the role and success of variation in country policy responses to date. 

# Bibliography


