---
title: "A Retrospective Bayesian Model for Measuring Covariate Effects on Observed COVID-19 Test and Case Counts"
author: |
  | Robert Kubinec
  | New York University Abu Dhabi
date: "April 7th, 2020"
toc: false
csl: nature.csl
output: 
  #bookdown::word_document2
  bookdown::pdf_document2:
    keep_tex: true
    includes:
      in_header:
          preamble.tex
bibliography: BibTexDatabase.bib
abstract: "As the COVID-19 outbreak increases, increasing numbers of researchers are examining how an array of factors either hurt or help the spread of the disease. Unfortunately, the majority of available data, primarily confirmed cases of COVID-19, are widely known to be biased indicators of the spread of the disease. In this paper I present a retrospective Bayesian model that is much simpler than epidemiological models of disease progression but is still able to identify the effect of covariates on the historical infection rate. I show that while observed cases and test counts cannot be used to measure the true infection rate due to confounding between the infection rate and the observed data, it is possible to use informative priors derived from susceptible-infected-recovered (SIR) models to estimate associations between background factors and the disease. The model is validated by comparing estimation of the count of infected to projections from expert surveys and extant disease forecasts. To apply the model, I show that as of April 7th, U.S. states with higher rates of cardiovascular deaths are associated with more rapidly rising COVID-19 infection rates, while a state's vote share for Donald Trump in the 2016 election is not.^[To reproduce the model and to access the underlying Stan code, please see my [Github page](https://github.com/saudiwin/corona_tscs). This paper is part of the [CoronaNet project](https://lumesserschmidt.github.io/CoronaNet/) collecting data on government responses to the COVID-19 pandemic. For helpful comments I thank Cindy Cheng, Joan Barcelo and Luiz Max Cavalho.]"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning=FALSE,message=FALSE)
require(dplyr)
require(tidyr)
require(ggplot2)
require(rstan)
require(stringr)
require(lubridate)
require(bayesplot)
require(historydata)
require(readr)
require(datasets)
require(extraDistr)
require(patchwork)
require(rstanarm) # for observed data modeling
require(tidycensus)

set.seed(662817)

rstan_options(auto_write=T)

knitr::opts_chunk$set(warning=F,message=F)

system2("git",args=c("-C ~/covid-tracking-data","pull"))
system2("git",args=c("-C ~/covid-19-data","pull"))

# whether to run model (it will take a few hours) or load saved model from disk

run_model <- F

pan_model <- stan_model("corona_tscs_betab.stan")

```


# Introduction

As more and more data has become available on observed case counts of the SARS-CoV2 coronavirus, there have been increasing attempts to infer how contextual factors like government policies, partisanship, and temperature affect the disease's spread [@carleton2020;@sajadi2020;@dudel2020;@tansim2020]. The temptation to make inferences from the observed data, however, can lead to misleading conclusions. For example, some policy makers have publicly questioned whether the predictions of epidemiological models are far worse than the observed case count.^[See article available at https://www.realclearpolitics.com/video/2020/03/26/dr_birx_coronavirus_data_d] By contrast, in this paper I show that the unobserved infection rate is a confounding variable affecting any estimates of covariates on the observed counts of COVID-19 cases and tests. For this reason, in this paper I present a retrospective Bayesian model that can adjust for this bias by estimating the unseen infection rate up to an unidentified constant. Furthermore, by incorporating informative priors from the susceptible-infected-recovered (SIR)/susceptible-exposed-infected-recovered (SEIR) papers on SARS-CoV2  [@peak2020;@riou2020;@verity2020;@perkins2020;@lourenco2020;@li2020;@ferguson2020], it is possible to put an informative prior on the unseen infection rate and estimate both recent disease trends and the effect of covariates on the historical spread of the disease.


In this paper, I show how the model can be applied by measuring the association between U.S. state-level factors and the disease as of April 7th, 2020, including the timing of state of emergency declarations, vote share in the 2016 election for President Donald Trump, gross domestic product (GDP), and health-related factors. I show that as of April 7th the timing of state of emergency declarations and vote share for President Donald Trump do not predict the number of infected individuals, while higher numbers of cardiovascular deaths, lower state income and lower air pollution are positively associated with infection rates.

# Model Overview

In this section I present an intuitive overview of the model, and I refer the interested reader to the supplemental materials for a more complete exposition combined with Monte Carlo simulations showing recovery of the latent infection rate. 

The compartmental model employed by epidemiologists to study disease, and in particular SARS-CoV2 [@peak2020;@riou2020;@verity2020;@perkins2020;@lourenco2020;@li2020;@ferguson2020], estimates different classes of individuals in the population, denoted $S$ for susceptible, $I$ for infected, and $R$ for recovered (other letters may be added such as $E$ for exposed). Using ordinary differential equations, the count of the infected can then be estimated by equating these three identities assuming a fixed population size, as seems reasonable during a relatively quick epidemic. These models guide our understanding of the disease and its progression, and have made warnings about the disease's spread that are proving true on a daily basis.

By contrast, this paper endeavors to estimate a much simpler quantity than the entire evolution of the outbreak. Many researchers and the general public often want to learn about what has already happened, or the *empirical* infection rate (also called the attack rate in the epidemiological literature). For a number of time points $t \in T$ since the outbreak's start and countries/regions $c \in C$, I aim to identify the following quantity:

$$
f_t \left (\frac{I_{ct}}{S_{ct}+R_{ct}} \right )
$$

Assuming a fixed population size, this quantity is simply the marginal rate of infections in the population up to the present. Because these are the quantities themselves, not their derivatives, there is no attempt to simultaneously solve for all the parameters. The function $f_t$ determines the historical time trend of the rate of infection (which is assumed to be same across countries/regions) in the population up to time $T$, the present. Because the denominator is shifting over time due to disease progression dynamics, this model is only useful for retrospection, i.e., to examine factors that may be influencing the empirical time trend $f_t$. As $S_{ct}$ and $R_{ct}$ are exogenous to the model, the model cannot predict future prevalence of the disease given that it does not determine these crucial factors. In other words, this model can be seen as a local linear approximation to the $I_{ct}$ curve from an SIR model.

However, we do not have estimates of the actual infected rate $I_{ct}$, only positive COVID-19 cases $a_{ct}$ and numbers of COVID-19 tests $q_{ct}$. Given this limitation, the aim of the model is to backwards infer the infection rate $I_{ct}$ as a latent process given observed test and counts. Modeling the latent process is necessary to avoid bias in using only observed case counts as a proxy for $I_{ct}$. The reason for this is shown in Figure \ref{tikzfig} in which a covariate $X_{ct}$, such as temperature, is hypothesized to affect the infection rate $I_{ct}$. Unfortunately, increasing infection rates can cause both increasing numbers of observed counts $a_{ct}$ and tests $q_{ct}$. As more people are infected, more tests are likely to be done, which will increase the number of cases independently of the infection rate. As a result, due to the back-door path from the infection rate $I_{ct}$ to case counts $a_{ct}$ via the number of tests $q_{ct}$, it is impossible to infer the effect of $X_{ct}$ on $I_{ct}$ from the observed data alone without modeling the latent infection rate. 

\begin{figure}
\label{tikzfig}
\caption{Directed Acyclic Graph Showing Confounding of Covariate $X_{ct}$ on Observed Tests $q_{ct}$ and Cases $a_{ct}$ Due to Unobserved Infection Rate $I_{ct}$}
\ctikzfig{policy_dag}
\footnotesize{Figure shows the relationship between a covariate $X_{ct}$ representing a policy or social factor influencing the infection rate $I_{ct}$. Because the infection rate $I_{ct}$ influences both the number of reported tests $q_{ct}$ and reported cases $a_{ct}$, any regression of a covariate $X_{ct}$ on the reported data will be biased.}
\end{figure}

The Bayesian model presented in the supplementary materials provides a full explanation of how to model the infection rate's influence on both cases and tests simultaneously. I further show in the supplementary materials that two restrictions are necessary to identify the sign and rank of the effect of covariates $X_{ct}$ on $I_{ct}$: the paths $I_{ct} \rightarrow q_{ct}$ and $I_{ct} \rightarrow a_{ct}$ must be strictly positive so that an increasing infection rate will have a non-decreasing effect on cases and tests. Given these restrictions and weakly informative priors on the parameters, it is possible to know whether $X_{ct}$ is associated with increasing or decreasing $I_{ct}$, though not with respect to the actual number of infected people, only in terms of the covariate's sign or rank relative to other covariates.

Furthermore, I show in the supplementary materials that if further information can be put on the ratio between the true number of infected individuals $I_{ct}$ and the number of tests $q_ct$, it is possible to transform inferences from the model to approximate counts of infected people up to the present. Thankfully, this information is available through epidemiological models of the disease, which offer inferences on the number of un-diagnosed cases [@li2020;@peak2020]. Based on these estimates, I can put an informative prior in the model that the number of tests is likely to be at least 10% of the total number of infected individuals.   With this prior information from disease simulations, it is possible to come up with an empirical estimate of infected rates and covariate effects that is more useful to the general public than solely observed tests and cases, as I demonstrate in the next section.


# Estimation with Data

The only data required to fit the model, in addition to estimates of the background covariates, are observed cases and tests for COVID-19 by day. In this section, I fit the model to numbers of COVID-19 case counts on US states and territories provided by [The New York Times](https://github.com/nytimes/covid-19-data). By doing so, we can use the differences in trajectories across states to help identify the effect of state-level covariates on the infection rate. I supplement these observed case counts with testing data by day from the [COVID-19 Tracking Project](https://github.com/COVID19Tracking/covid-tracking-data). The testing data starts at March 4th, so I impute the testing data back in time by assuming that the average case/tests ratio stays the same to the origin of the outbreak. Furthermore, as there are discrepancies where the reported number of tests in some states like New Jersey is less than the total number of cases, I impute the number of tests via the case/test ratio for the sample as a whole.

To analyze the effect of suppression policies, I proxy for preparedness to fight the epidemic by including the date that states of emergencies were declared across U.S. states and territories in the model.^[See this site for dates and relevant sources: https://en.wikipedia.org/wiki/U.S._state_and_local_government_response_to_the_2020_coronavirus_pandemic] The suppression covariate is then equal to a vector of days which is mean centered and standardized. I further add in state-level data on Donald Trump's vote share for the 2016 election from the MIT Election Lab, a 2019 estimate of state GDP from the Bureau of Economic Analysis, the 2018 percentage of foreign born residents from the U.S. Census Bureau, and 2019 state-level average data on air pollution, cardiovascular deaths per capita, dedicated health care providers, public health funding, preventable hospitalizations, and smoking rates provided by the United Health Foundation.

In this section I first fit a partially-identified model without any information about the true number of infected people to demonstrate that it is possible to obtain estimates of covariates, though with substantial uncertainty. I then show how we can use insight from SIR/SEIR models to add in informative prior information on the likely number of infected people and translate the estimates into probable infection rates. I then show that these estimates in fact closely track the predictions of SIR/SEIR models for the U.S. population from March 2020, providing external validity for the method and for these predictions.



```{r munge_data,include=F}

# vote share
# MIT Election Lab
load("data/mit_1976-2016-president.rdata")

vote_share <- filter(x,candidate=="Trump, Donald J.",
                     party=="republican",
                     writein=="FALSE") %>% 
  mutate(trump=candidatevotes/totalvotes)

# state GDP

state_gdp <- readxl::read_xlsx("data/qgdpstate0120_0_bea.xlsx",sheet="Table 3") %>% 
  mutate(gdp=Q1 + Q2 + Q3 +Q4)

# US Census data - population & percent foreign-born
# note: you need a Census API key loaded to use this -- see package tidycensus docs

acs_data <- get_acs("state",variables=c("B01003_001","B05002_013"),year=2018,survey="acs1") %>% 
  select(-moe) %>% 
  mutate(variable=recode(variable,
                         B01003_001="state_pop",
                         B05002_013="foreign_born")) %>% 
  spread("variable","estimate") %>% 
  mutate(prop_foreign=foreign_born/state_pop)


# health data

health <- read_csv("data/2019-Annual.csv") %>% 
  filter(`Measure Name` %in% c("Air Pollution","Cardiovascular Deaths","Dedicated Health Care Provider",
                              "Population under 18 years", "Public Health Funding","Smoking")) %>% 
  select(`Measure Name`,state="State Name",Value) %>% 
  distinct %>% 
  spread(key="Measure Name",value="Value")

merge_names <- tibble(state.abb,
                      state=state.name)

nyt_data <- read_csv("~/covid-19-data/us-states.csv") %>% 
  complete(date,state,fill=list(cases=0,deaths=0,fips=0)) %>% 
  mutate(month_day=ymd(date)) %>% 
  group_by(state) %>% 
    arrange(state,date) %>% 
  mutate(Difference=cases - dplyr::lag(cases),
         Difference=coalesce(Difference,0,0)) %>% 
  left_join(merge_names,by="state")

tests <- read_csv("~/covid-tracking-data/data/states_daily_4pm_et.csv") %>% 
  mutate(month_day=ymd(date)) %>% 
  arrange(state,month_day) %>% 
  group_by(state) %>% 
  mutate(tests_diff=total-dplyr::lag(total),
         cases_diff=positive-dplyr::lag(positive),
         cases_diff=coalesce(cases_diff,positive),
         cases_diff=ifelse(cases_diff<0,0,cases_diff),
         tests_diff=coalesce(tests_diff,total),
         tests_diff=ifelse(tests_diff<0,0,tests_diff)) %>% 
  select(month_day,tests="tests_diff",total,state.abb="state")

# merge cases and tests

combined <- left_join(nyt_data,tests,by=c("state.abb","month_day")) %>% 
  left_join(acs_data,by=c("state"="NAME")) %>% 
  filter(!is.na(state_pop))

# add suppression data

emergency <- read_csv("data/state_emergency_wikipedia.csv") %>% 
  mutate(day_emergency=dmy(paste0(`State of emergency declared`,"-2020")),
         mean_day=mean(as.numeric(day_emergency),na.rm=T),
         sd_day=sd(as.numeric(day_emergency),na.rm=T),
         day_emergency=((as.numeric(day_emergency) - mean_day)/sd_day)) %>% 
  select(state="State/territory",day_emergency,mean_day,sd_day) %>% 
  mutate(state=substr(state,2,nchar(state))) %>% 
  filter(!is.na(day_emergency))

combined <- left_join(combined,emergency,by="state")

# add in other datasets 

combined <- left_join(combined,health,by="state")
combined <- left_join(combined,select(state_gdp,state,gdp),by="state")
combined <- left_join(combined,select(vote_share,state,trump))

# impute data

combined <- group_by(combined,state) %>% 
  mutate(test_case_ratio=sum(tests,na.rm=T)/sum(Difference,na.rm=T)) %>% 
  ungroup %>% 
  mutate(test_case_ratio=ifelse(test_case_ratio<1 | is.na(test_case_ratio),
                                mean(test_case_ratio[test_case_ratio>1],na.rm=T),test_case_ratio)) %>% 
  group_by(state) %>% 
    mutate(tests=case_when(Difference>0 & is.na(tests)~Difference*test_case_ratio,
                    Difference==0~0,
                    Difference>tests~Difference*test_case_ratio,
                    TRUE~tests)) %>% 
  arrange(state) %>% 
  filter(state!="Puerto Rico")

combined <- group_by(combined,state) %>% 
  arrange(month_day) %>% 
  mutate(outbreak=as.numeric(cases>1)) %>% 
  fill(outbreak,.direction="down") %>% 
  mutate(outbreak_time=cumsum(outbreak)) %>% 
  ungroup %>%
  mutate_at(c("Cardiovascular Deaths",
              "outbreak_time",
              "Air Pollution",
              "Dedicated Health Care Provider",
              "Smoking",
              "Population under 18 years",
              "Public Health Funding",
              "gdp",
              "trump",
              "day_emergency",
              "prop_foreign"), ~as.numeric(scale(.))) %>% 
  group_by(month_day) %>% 
  mutate(world_infect=sum(outbreak_time>1))

# create case dataset

cases_matrix <- select(combined,Difference,month_day,state) %>% 
  group_by(month_day,state) %>% 
  summarize(Difference=as.integer(mean(Difference))) %>% 
  spread(key = "month_day",value="Difference")

cases_matrix_num <- as.matrix(select(cases_matrix,-state))

# create tests dataset

tests_matrix <- select(combined,tests,month_day,state) %>% 
  group_by(month_day,state) %>% 
  summarize(tests=as.integer(mean(tests))) %>% 
  spread(key = "month_day",value="tests")

tests_matrix_num <- as.matrix(select(tests_matrix,-state))

# need the outbreak matrix

outbreak_matrix <- as.matrix(lapply(1:ncol(cases_matrix_num), function(c) {
  if(c==1) {
    outbreak <- as.numeric(cases_matrix_num[,c]>0)
  } else {
    outbreak <- as.numeric(apply(cases_matrix_num[,1:c],1,function(col) any(col>0)))
  }
  tibble(outbreak)
}) %>% bind_cols)

colnames(outbreak_matrix) <- colnames(cases_matrix_num)

time_outbreak_matrix <- t(apply(outbreak_matrix,1,cumsum))

just_data <- distinct(select(ungroup(combined),state,day_emergency,state_pop,trump,air="Air Pollution",
                      heart="Cardiovascular Deaths",
                      providers="Dedicated Health Care Provider",
                      young="Population under 18 years",
                      smoking="Smoking",
                      gdp,
                      public_health="Public Health Funding",
                      prop_foreign)) %>% arrange(state)

covs <- select(ungroup(just_data),-state,-state_pop) %>% as.matrix

# now give to Stan

ortho_time <- poly(scale(1:ncol(cases_matrix_num)),degree=3)

time_outbreak_center <- matrix(scale(c(time_outbreak_matrix)),nrow=nrow(time_outbreak_matrix),
                                                   ncol=ncol(time_outbreak_matrix))

real_data <- list(time_all=ncol(cases_matrix_num),
                 num_country=nrow(cases_matrix_num),
                 S=ncol(covs),
                 country_pop=floor(just_data$state_pop/100),
                 cases=cases_matrix_num,
                 ortho_time=ortho_time,
                 phi_scale=.01,
                 count_outbreak=as.numeric(scale(apply(outbreak_matrix,2,sum))),
                 tests=tests_matrix_num,
                 time_outbreak=time_outbreak_matrix,
                 time_outbreak_center=time_outbreak_center,
                 suppress=covs)

init_vals <- function() {
  list(phi_raw=c(30,10),
       world_infect=0.5,
       finding=0.5,
       alpha=c(0,-10))
}



if(run_model) {
  us_fit <- sampling(pan_model,data=real_data,chains=3,cores=3,iter=1500,warmup=1000,
                   init=init_vals)
  
  saveRDS(us_fit,"data/us_fit.rds")
} else {
  us_fit <- readRDS("data/us_fit.rds")
}


```

```{r infectstate,fig.cap="5% to 95% HPD Uncertainty Intervals of Partially-Identified Infection Rates by U.S. State with Total Average"}
all_est_state <- as.data.frame(us_fit,"num_infected_high") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="variable",value="estimate",-iter) %>% 
  group_by(variable) %>% 
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")),
         time_point=as.numeric(str_extract(variable,"[1-9][0-9]?0?(?=\\])")),
         time_point=ymd(min(combined$month_day)) + days(time_point-1))

all_est_state <- left_join(all_est_state,tibble(state_num=1:nrow(cases_matrix),
                                                state=cases_matrix$state,
                                                state_pop=real_data$country_pop,
                                    suppress_measures=real_data$suppress,by="state_num"))

# merge in total case count

case_count <- gather(cases_matrix,key="time_point",value="cases",-state) %>% 
  mutate(time_point=ymd(time_point)) %>% 
  group_by(state) %>% 
  arrange(state,time_point) %>% 
  mutate(cum_sum_cases=cumsum(cases)) 

us_case_count <- group_by(case_count,time_point) %>% 
  summarize(all_cum_sum=sum(cum_sum_cases))

all_est_state <- left_join(all_est_state,us_case_count,by="time_point")


all_est_state %>% 
  mutate(estimate=estimate) %>% 
  group_by(state_num,time_point,suppress_measures) %>% 
    summarize(med_est=quantile(estimate,.5),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>% 
  ggplot(aes(y=med_est,x=time_point)) +
  #geom_line(aes(group=state_num,colour=suppress_measures),alpha=0.5) +
  geom_ribbon(aes(ymin=low_est,
  ymax=high_est,
  group=state_num,
  fill=suppress_measures),alpha=0.5) +
  stat_smooth(colour="black") +
  theme_minimal() +
  scale_color_distiller(palette="RdBu",direction=-1) +
  ylab("Latent Infection Scale") +
  labs(caption="5% - 95% HPD Intervals are colored by\nwhen a state declared a state of emergency (standardized count of days).\nAs the total number of infected people is unknown,\nthis chart measures the relative marginal infection rates between states.") +
  xlab("Days Since Outbreak Start") +
  geom_hline(yintercept = 0,linetype=3) +
  guides(fill=guide_colorbar(title="Timing of State Emergency Declaration")) +
  theme(panel.grid = element_blank(),
        legend.position = "bottom")
ggsave("uncertain_state_rates.png")

```

```{r rankcountries1}

rank1 <- all_est_state %>% 
  group_by(state,iter) %>% 
  summarize(estimate=mean(estimate)) %>% 
  ungroup %>% 
  group_by(iter) %>% 
  mutate(rank_state=51 - rank(estimate),
         model="Partially Identified") %>% 
  group_by(state,model) %>% 
    summarize(med_est=mean(rank_state),
            high_est=quantile(rank_state,.95),
            low_est=quantile(rank_state,.05))
  

```

Figure \@ref(fig:infectstate) shows the 5% - 95% high posterior density (HPD) intervals of the latent infection rate by state since January 1st. The intervals are shaded by the relative time when a state declared a state of emergency, which reveals that state of emergency declarations are correlated with higher infection rates. As can be seen, there is a sharp discontinuity in the plot around March 1st when infection rates began to increase. While it would appear that the rate of increase has leveled off in the last week, that is not a supported inference as the scale is the logit scale, so it is similar to the log scale in that higher numbers are farther away than they appear visually. Because the latent scale is not identified, the figure is only showing how the infection rates have evolved from zero to the true but unknown top infection rate. As such, it appears to be slowing as it reaches the top of the scale, but that is simply an illusion of logarithmic growth. 


```{r suppress2,echo=F}

suppress_effect <- as.data.frame(us_fit,"suppress_effect") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="parameter",value="estimate",-iter) %>% 
  mutate(supp_type=as.numeric(str_extract(parameter,"(?<=\\[)[1-9][0-9]?0?")),
         variable=as.numeric(str_extract(parameter,"[1-9]?0?(?=\\])")))

num_infected <- as.data.frame(us_fit,"num_infected_high") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="variable",value="estimate",-iter) %>% 
  group_by(variable) %>% 
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")),
         time_point=as.numeric(str_extract(variable,"[1-9][0-9]?0?(?=\\])")),
         time_point=ymd(min(combined$month_day)) + days(time_point-1))

# iterate this bugger

p_infected <- group_by(num_infected,iter) %>% 
  summarize(est=mean(dlogis(estimate)))

if(run_model) {
    over_all <- parallel::mclapply(sample(1:max(num_infected$iter),100), function(q) {
    over_supp <- lapply(1:max(suppress_effect$variable), function(s) {
      over_time_level <- lapply(seq(min(time_outbreak_center),max(time_outbreak_center),length.out=100), function(i) {
        this_eff <- p_infected$est[q] * (suppress_effect$estimate[suppress_effect$supp_type==1 & suppress_effect$variable==s & suppress_effect$iter==q] + suppress_effect$estimate[suppress_effect$supp_type==2 & suppress_effect$variable==s & suppress_effect$iter==q]*i)
      
      tibble(estimate=this_eff,
             time_scale_point=i,
             iter=q,
             variable=s)
      }) %>% bind_rows
      
      over_time_level
      
    }) %>% bind_rows
    
    return(over_supp)
  },mc.cores=3) %>% bind_rows
    
    saveRDS(over_all,"data/over_all.rds")
    
} else {
  over_all <- readRDS("data/over_all.rds")
}




over_all_time <- group_by(over_all,variable,time_scale_point) %>% 
  summarize(med_est=median(estimate),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>% 
  mutate(model="Partially\nIdentified")


over_all_sum <- group_by(over_all,variable) %>% 
  summarize(med_est=median(estimate),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>% 
  mutate(model="Partially\nIdentified")
  
  

# calculate marginal effects

over_all_time <- left_join(over_all_time,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>% 
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      day_emergency="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

over_all_sum <- left_join(over_all_sum,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>% 
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      day_emergency="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

p1 <- over_all_sum %>%
  ggplot(aes(y=med_est,x=reorder(label,med_est))) +
  geom_pointrange(aes(ymin=low_est,ymax=high_est),alpha=0.8) +
  theme_minimal() +
  xlab("")  +
  ylab("Effect on Proportion Infected") +
  geom_hline(yintercept=0,linetype=2) +
  ggtitle("Constant Effect on\nProportion Infected") +
  theme(panel.grid=element_blank(),
        plot.title = element_text(size=10,hjust=0.5)) +
  coord_flip()

p2 <- over_all_time %>%
 ggplot(aes(y=med_est,x=time_scale_point)) +
    xlab("Time Since Outbreak Began")  +
  ylab("Effect on Proportion Infected") +
  geom_ribbon(aes(ymin=low_est,ymax=high_est),alpha=0.5,fill="red") +
  geom_line(linetype=2) +
  ggtitle("Time-Varying Effect on\nProportion Infected") +
  theme_minimal() +
  geom_hline(yintercept=0,linetype=3) +
  theme(panel.grid=element_blank(),
        plot.title = element_text(size=10,hjust=0.5)) +
  facet_wrap(~label,scales="free_y")

p1 + p2

ggsave("marginal_noid.png")

```

```{r run_over_id_model_scale,include=F}


if(run_model) {
  pan_model_scale <- stan_model("corona_tscs_betab_scale.stan")
  
  us_fit_scale <- sampling(pan_model_scale,data=real_data,chains=3,cores=3,iter=1500,warmup=1000,
                           control=list(adapt_delta=0.95),
                   init=init_vals)
  
  saveRDS(us_fit_scale,"data/us_fit_scale.rds")
} else {
  us_fit_scale <- readRDS("data/us_fit_scale.rds")
}


```


```{r naivemodel,include=F}

# calculate the same info but only with observed data

if(run_model) {
  obs_model <- stan_glm(cbind(combined$Difference,floor(combined$state_pop/100) - combined$Difference) ~ poly(outbreak_time,3)[,2:3] +
                     day_emergency*poly(outbreak_time,1)[,1] +
                        `Cardiovascular Deaths`*poly(outbreak_time,1)[,1] +
                        `Air Pollution`*poly(outbreak_time,1)[,1] +
                        `Dedicated Health Care Provider`*poly(outbreak_time,1)[,1] +
                        `Smoking`*poly(outbreak_time,1)[,1] +
                        `Population under 18 years`*poly(outbreak_time,1)[,1] +
                     prop_foreign*poly(outbreak_time,1)[,1] +
                        `Public Health Funding`*poly(outbreak_time,1)[,1] +
                        gdp*poly(outbreak_time,1)[,1] +
                        trump*poly(outbreak_time,1)[,1] +
                     world_infect,
                      data=combined,
                 QR=TRUE,
                      family="binomial",
                 chains=1,cores=1)

saveRDS(obs_model,"data/obs_model.rds")
} else {
  obs_model <- readRDS("data/obs_model.rds")
}



out_data <- mcmc_intervals_data(obs_model,regex_pars=":") %>% 
  filter(!grepl(x=parameter,pattern="I"))

```



```{r infectscaled,fig.cap="Approximate Total Number of COVID-19 Infected Individuals in the U.S. as of April 7th",fig.height=4}
all_est_state <- as.data.frame(us_fit_scale,"num_infected_high") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="variable",value="estimate",-iter) %>% 
  group_by(variable) %>% 
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")),
         time_point=as.numeric(str_extract(variable,"[1-9][0-9]?0?(?=\\])")),
         time_point=ymd(min(combined$month_day)) + days(time_point-1))

all_est_state <- left_join(all_est_state,tibble(state_num=1:nrow(cases_matrix),
                                                state=cases_matrix$state,
                                                state_pop=real_data$country_pop,
                                    suppress_measures=real_data$suppress,by="state_num"))

# merge in total case count

case_count <- gather(cases_matrix,key="time_point",value="cases",-state) %>% 
  mutate(time_point=ymd(time_point)) %>% 
  group_by(state) %>% 
  arrange(state,time_point) %>% 
  mutate(cum_sum_cases=cumsum(cases)) 

us_case_count <- group_by(case_count,time_point) %>% 
  summarize(all_cum_sum=sum(cum_sum_cases))

all_est_state <- left_join(all_est_state,us_case_count,by="time_point")

calc_sum <- all_est_state %>% 
  ungroup %>% 
  mutate(estimate=(plogis(estimate)/100)*(state_pop*100)) %>% 
  group_by(state_num,iter) %>% 
  arrange(state_num,time_point) %>% 
  mutate(cum_est=cumsum(estimate)) %>% 
  group_by(time_point,iter,all_cum_sum) %>% 
  summarize(us_total=sum(cum_est)) %>% 
  group_by(time_point,all_cum_sum) %>% 
  summarize(med_est=quantile(us_total,.5),
            high_est=quantile(us_total,.95),
            low_est=quantile(us_total,.05)) 

max_est <- as.integer(round(calc_sum$med_est[calc_sum$time_point==max(calc_sum$time_point)]))
high_max_est <- as.integer(round(calc_sum$high_est[calc_sum$time_point==max(calc_sum$time_point)]))
low_max_est <- as.integer(round(calc_sum$low_est[calc_sum$time_point==max(calc_sum$time_point)]))
max_obs <- calc_sum$all_cum_sum[calc_sum$time_point==max(calc_sum$time_point)]

options(scipen=999)

# load expert survey results

expert_survey <- read_csv("data/consensusForecastsDB.csv") %>% 
  filter(questionLabel %in% c("QF5","QF4","QF3"),
         surveyIssued>ymd("2020-03-16")) %>% 
    mutate(keep=case_when(surveyIssued==ymd("2020-03-02")~"QF4",
                        surveyIssued==ymd("2020-03-09")~"QF6",
                        surveyIssued==ymd("2020-03-16")~"QF4",
                        surveyIssued==ymd("2020-03-23")~"QF4",
                        surveyIssued==ymd("2020-03-30")~"QF3",
                        TRUE~"reject")) %>% 
  filter(questionLabel==keep,cumprob>0.05,cumprob<0.95) %>% 
  group_by(surveyIssued,questionLabel) %>% 
  summarize(med_est=bin[abs(cumprob-0.5)==min(abs(cumprob-0.5))],
            low_est=bin[abs(cumprob-0.1)==min(abs(cumprob-.1))],
            high_est=bin[abs(cumprob-0.9)==min(abs(cumprob-.9))]) %>% 
  rename(time_point="surveyIssued")


# need to add in cumulative case counts

calc_sum %>% 
  ggplot(aes(y=med_est,x=time_point)) +
  geom_ribbon(aes(ymin=low_est,
  ymax=high_est),
  fill="blue",
  alpha=0.5) +
  geom_line(aes(y=all_cum_sum)) +
  theme_minimal() +
  ylab("Total Number Infected/Reported") +
  scale_y_continuous(labels=scales::comma) +
  labs(caption="Blue 5% - 95% HPD intervals show estimated infected and the black line\nshows observed cases from the New York Times.\nThese estimates are based on the assumption that as few as 10% of cases\nmay be reported based on SIR/SEIR models.") +
  annotate("text",x=ymd(c("2020-04-07","2020-04-07")),
           y=c(max_est,max_obs),
           hjust=1,
           vjust=0,
           fontface="bold",
           size=3,
           label=c(paste0("Estimated Infected:\n",formatC(low_max_est,big.mark=",",format = "f",digits=0)," - ",
                                                         formatC(high_max_est,big.mark=",",format = "f",digits=0)),
                   paste0("Total Reported Cases:\n",formatC(max_obs,big.mark=",")))) +
  annotate("text",x=ymd(c("2020-03-01","2020-03-12",
                          as.character(expert_survey$time_point))),
           y=c(300000,180000,expert_survey$high_est*1.01),
           vjust=0,
           fontface="bold",
           size=3,
           label=c("Li et al. March 8th",
                   "Perkins et al. March 26th",
                   paste0("Expert Survey\n",expert_survey$time_point)),alpha=0.8) +
  # previously published annotations
  annotate("pointrange",x=ymd("2020-03-01"),y=9001,ymin=2299,ymax=20403,alpha=0.5) +
  annotate("pointrange",x=ymd("2020-03-12"),y=22876,ymin=7451,ymax=53044,alpha=0.5) +
  geom_pointrange(data=expert_survey,aes(ymin=low_est,ymax=high_est),alpha=0.5) +
  xlab("Days Since Outbreak Start") +
  theme(panel.grid = element_blank(),
        legend.position = "top")

ggsave("est_vs_obs_experts.png")

```

By comparioson Figures \@ref(fig:infectscaled) and \@ref(fig:stateplot) show fully-identified models incorporating informative prior information suggesting that the ratio of tests to infected ratio individuals is probably no less than 10% of those infected. This information, as previously mentioned, was derived from simulation and statistical modeling of COVID-19 outbreaks so far suggesting that a large proportion of infected individuals are undetected [@li2020;@peak2020]. Based on this information, the scale of the latent infection process shown in Figure \@ref(fig:infectstate) can be further identified. Figure \@ref(fig:infectscaled) shows that the likely cumulative number of infected cases, including those who may have recovered or died, is likely approaching 1 million infected individuals in the United States, in line with SIR/SEIR projections released recently.^[See https://www.nytimes.com/2020/04/01/world/coronavirus-news.html?action=click&module=Spotlight&pgtype=Homepage for a recent overview.]  Furthermore, Figure \@ref(fig:stateplot) shows significant state by state heterogeneity, with New York showing the greatest number of infected, followed by California. There is some suggestive evidence that California's infected count growth may be slowing, though the large uncertainty interval suggests that this inference would be unwise without more data or assumptions.


```{r stateplot,fig.cap="Average Cumulative Count of Infected People by U.S. State as of March 31st",fig.height=4,echo=F}
require(ggrepel)

calc_sum_state <- all_est_state %>% 
  ungroup %>% 
  mutate(estimate=(plogis(estimate)/100)*(state_pop*100)) %>% 
  group_by(state,iter) %>% 
  arrange(state,time_point) %>% 
  mutate(cum_est=cumsum(estimate)) %>% 
  group_by(time_point,state,suppress_measures) %>% 
  summarize(med_est=quantile(cum_est,.5),
            high_est=quantile(cum_est,.95),
            low_est=quantile(cum_est,.05)) 

# Annotations

# get top 5 plus random 5 

top_5 <- filter(calc_sum_state,time_point==max(calc_sum_state$time_point)) %>% 
  arrange(desc(med_est)) %>% 
  ungroup %>% 
  slice(c(1:5,sample(6:length(unique(calc_sum_state$state)),5))) %>% 
  distinct %>% 
  mutate(label=paste0(state,":",formatC(low_est,big.mark=",",format = "f",digits=0)," - ",
                                                         formatC(high_est,big.mark=",",format = "f",digits=0)))

calc_sum_state %>% 
  ggplot(aes(y=med_est,x=time_point)) +
  geom_line(aes(group=state,colour=med_est)) +
  # geom_ribbon(aes(ymin=low_est,
  # ymax=high_est,
  # group=state_num,
  # fill=suppress_measures),alpha=0.5) +
  theme_minimal() +
  scale_color_distiller(palette="Reds",direction=1) +
  ylab("Cumulative Count") +
  labs(caption="Some lines are labeled with uncertainty of estimates (5% - 95% Interval).\nThese estimates are based on the assumption that as few as\n10% of cases may be reported based on SIR/SEIR models. Does not exclude people\nwho may have recovered or died.") +
  geom_text_repel(data=top_5,aes(x=time_point,y=med_est,label=label),
                  size=3,fontface="bold",segment.colour = NA) +
  scale_y_continuous(labels=scales::comma) +
  xlab("Days Since Outbreak Start") + 
  guides(colour="none") +
  theme(panel.grid = element_blank(),
        legend.position = "top")
ggsave("certain_state_rates.png")
```


```{r rankcountries2,fig.cap="Comparison of Identified and Partially-Identified Models' Ranking of Infected by State",fig.height=4}

# rank2 <- all_est_state %>% 
#   group_by(state,iter) %>% 
#   summarize(estimate=mean(estimate)) %>% 
#   ungroup %>% 
#   group_by(iter) %>% 
#   mutate(rank_state=51 - rank(estimate),
#          model="Identified") %>% 
#   group_by(state,model) %>% 
#     summarize(med_est2=mean(rank_state))
# 
# bind_cols(rank1,rank2) %>% 
#   ggplot(aes(y=med_est,x=med_est2)) +
#   geom_point(alpha=0.8,colour="blue") + 
#   theme_minimal() +
#   xlab("")  +
#   scale_colour_brewer(type="qual") +
#   xlab("Partially Identified State Rank\nof Average Proportion Infected") +
#   ylab("Fully Identified State Rank\nof Average Proportion Infected") +
#   geom_smooth(method="lm") +
#   geom_abline(yintercept=0,slope=1,linetype=2) +
#   theme(panel.grid=element_blank(),
#         plot.title = element_text(size=10,hjust=0.5)) +
#   coord_flip()
  

```

Figure \@ref(fig:compmarg) shows the effect of covariates in the model. Each covariate has two kinds of effects, a static effect on the left-hand side and a time-varying effect on the right-hand side, where time is coded as a linear counter since the start of the outbreak (at least one case recorded) in each state. As can be seen, even though the latent scale is only partially identified, the effect signs and effect sizes between the partially-identified and fully-identified models are very close, and the uncertainty intervals always overlap. As such, while the fully-identified model is certainly preferred, in cases where such epidemiological information is not available, the partially-identified model is a reasonable substitute, especially when the only alternative is using the observed case counts without statistical correction.

```{r suppress1,echo=F}

suppress_effect <- as.data.frame(us_fit_scale,"suppress_effect") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="parameter",value="estimate",-iter) %>% 
  mutate(supp_type=as.numeric(str_extract(parameter,"(?<=\\[)[1-9][0-9]?0?")),
         variable=as.numeric(str_extract(parameter,"[1-9]?0?(?=\\])")))

num_infected <- as.data.frame(us_fit_scale,"num_infected_high") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="variable",value="estimate",-iter) %>% 
  group_by(variable) %>% 
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")),
         time_point=as.numeric(str_extract(variable,"[1-9][0-9]?0?(?=\\])")),
         time_point=ymd(min(combined$month_day)) + days(time_point-1))

# iterate this bugger


p_infected <- group_by(num_infected,iter) %>% 
  summarize(est=mean(dlogis(estimate)))

if(run_model) {
    over_all2 <- parallel::mclapply(sample(1:max(num_infected$iter),100), function(q) {
    over_supp <- lapply(1:max(suppress_effect$variable), function(s) {
      over_time_level <- lapply(seq(min(time_outbreak_center),max(time_outbreak_center),length.out=100), function(i) {
        this_eff <- p_infected$est[q] * (suppress_effect$estimate[suppress_effect$supp_type==1 & suppress_effect$variable==s & suppress_effect$iter==q] + suppress_effect$estimate[suppress_effect$supp_type==2 & suppress_effect$variable==s & suppress_effect$iter==q]*i)
      
      tibble(estimate=this_eff,
             time_scale_point=i,
             iter=q,
             variable=s)
      }) %>% bind_rows
      
      over_time_level
      
    }) %>% bind_rows
    
    return(over_supp)
  },mc.cores=3) %>% bind_rows
    
    saveRDS(over_all2,"data/over_all2.rds")
    
} else {
  over_all2 <- readRDS("data/over_all2.rds")
}




over_all_time2 <- group_by(over_all2,variable,time_scale_point) %>% 
  summarize(med_est=median(estimate),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>% 
  mutate(model="Fully\nIdentified")


over_all_sum2 <- group_by(over_all2,variable) %>% 
  summarize(med_est=median(estimate),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>% 
  mutate(model="Fully\nIdentified")
  
  

# calculate marginal effects

over_all_time2 <- left_join(over_all_time2,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>% 
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      day_emergency="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

over_all_sum2 <- left_join(over_all_sum2,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>% 
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      day_emergency="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

p3 <- over_all_sum2 %>%
  ggplot(aes(y=med_est,x=reorder(label,med_est))) +
  geom_pointrange(aes(ymin=low_est,ymax=high_est),alpha=0.8) +
  theme_minimal() +
  xlab("")  +
  ylab("Effect on Proportion Infected") +
  geom_hline(yintercept=0,linetype=2) +
  ggtitle("Constant Effect on\nProportion Infected") +
  theme(panel.grid=element_blank(),
        plot.title = element_text(size=10,hjust=0.5)) +
  coord_flip()

p4 <- over_all_time2 %>%
 ggplot(aes(y=med_est,x=time_scale_point)) +
    xlab("Time Since Outbreak Began")  +
  ylab("Effect on Proportion Infected") +
  geom_ribbon(aes(ymin=low_est,ymax=high_est),alpha=0.5,fill="red") +
  geom_line(linetype=2) +
  ggtitle("Time-Varying Effect on\nProportion Infected") +
  theme_minimal() +
  geom_hline(yintercept=0,linetype=3) +
  theme(panel.grid=element_blank(),
        plot.title = element_text(size=10,hjust=0.5)) +
  facet_wrap(~label,scales="free_y")

p3 + p4

ggsave("marginal_id.png")

```

Figure \@ref(fig:rankcountries2) compares the identified and partially-identified models' infection rates by plotting the average posterior rank of infection for each state and each model. As can be seen, while there is stochastic noise, there is a very clear and strong relationship in terms of the ranks, with a linear fit very close to the 45-degree horizontal line indicating a 1:1 relationship. While the models' predictions differ, they are estimating the same latent quantity, albeit with noise.

As can be seen in the plot B of Figure \@ref(fig:compmarg), cardiovascular deaths has the largest positive over-time effect in both models, while smoking is also of reasonable size in the fully-identified model, though the uncertainty interval includes zero. Conversely, states with higher GDP and worse air quality do not seem to have as many infections; the reasons for this inference could be due to a variety of reasons related to where the outbreak in the United States happened to start, i.e., in urban areas on the coasts. Whether this association will hold up over time as the outbreak spreads is an open question. 

What is important to note is that Trump vote share is not associated with increasing COVID-19 infections, despite public opinion polling showing that Americans who are more conservative tend to discount the danger posed by the virus.^[See https://www.vox.com/2020/3/15/21180506/coronavirus-poll-democrats-republicans-trump.] In addition, those states that declared an earlier state of emergency have not yet witnessed a slowing infection rate. Again, as the infection increases, there will be more data to make inter-state comparisons and obtain more information about whether these associations will hold up for the course of the virus. At present, however, there is no reason to believe that such an association exists.


Plot A of Figure \@ref(fig:compmarg), by comparison, does not reveal many inter-state differences. The partially identified model shows that air quality also has a constant effect, but the fully identified model does not, so it would be better in this instance to not infer that air quality also has a constant effect. These differences in base rates between the two models are likely due to scaling artifacts associated with the lack of identification of the latent scale. It is important to note as well that the fully-identified model generally has smaller uncertainty intervals.


```{r compmarg,fig.cap="Comparison of Effects from Identified and Partially-Identified Models"}

plot_data1 <- mcmc_intervals_data(us_fit,regex_pars ="effect") %>% 
  mutate(supp_type=as.numeric(str_extract(parameter,"(?<=\\[)[1-9][0-9]?0?")),
         variable=as.numeric(str_extract(parameter,"[1-9]?0?(?=\\])")))

# join back to labels 

plot_data1 <- left_join(plot_data1,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>% 
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      day_emergency="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

plot_data2 <- mcmc_intervals_data(us_fit_scale,regex_pars ="effect") %>% 
  mutate(supp_type=as.numeric(str_extract(parameter,"(?<=\\[)[1-9][0-9]?0?")),
         variable=as.numeric(str_extract(parameter,"[1-9]?0?(?=\\])")))

# join back to labels 

plot_data2 <- left_join(plot_data2,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>% 
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      day_emergency="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

plot_data3 <- mcmc_intervals_data(obs_model,pars=c("day_emergency",
                                                   "trump",
                                                   "`Air Pollution`",
                                                   "`Cardiovascular Deaths`",
                                                   "`Dedicated Health Care Provider`",
                                                   "`Population under 18 years`",
                                                   "Smoking",
                                                   "gdp",
                                                   "prop_foreign",
                                                   "`Public Health Funding`",
                                                   "day_emergency:poly(outbreak_time, 1)[, 1]",
                                                   "poly(outbreak_time, 1)[, 1]:trump",
                                                   "poly(outbreak_time, 1)[, 1]:`Air Pollution`",
                                                   "poly(outbreak_time, 1)[, 1]:`Cardiovascular Deaths`",
                                                   "poly(outbreak_time, 1)[, 1]:`Dedicated Health Care Provider`",
                                                   "poly(outbreak_time, 1)[, 1]:`Population under 18 years`",
                                                   "poly(outbreak_time, 1)[, 1]:Smoking",
                                                   "poly(outbreak_time, 1)[, 1]:gdp",
                                                   "poly(outbreak_time, 1)[, 1]:prop_foreign",
                                                   "poly(outbreak_time, 1)[, 1]:`Public Health Funding`")) %>% 
  mutate(supp_type=rep(c(1,2),each=10),
         variable=rep(1:10,2))

# join back to labels 

plot_data3 <- left_join(plot_data3,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>% 
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      day_emergency="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

plot_data <- bind_rows(list(`Latent\nInfection Rate`=plot_data2,
                            `Observed\nCases`=plot_data3),
                       .id="model")

p1 <- plot_data %>% 
  filter(supp_type==1) %>% 
  ggplot(aes(y=m,x=reorder(label,m))) +
  geom_pointrange(aes(ymin=ll,ymax=hh,colour=model),alpha=0.8,position=position_dodge(width=0.5),size=.5) +
  theme_minimal() +
  xlab("")  +
  ylab("Effect on Proportion Infected") +
  geom_hline(yintercept=0,linetype=2) +
  scale_color_brewer(type="qual") +
  ggtitle("Constant Effect on\nProportion Infected") +
  theme(panel.grid=element_blank(),
        plot.title = element_text(size=10,hjust=0.5),
        legend.position = "top") +
    guides(color=guide_legend(title="")) +
  coord_flip()

p2 <- plot_data %>% 
  filter(supp_type==2) %>% 
  ggplot(aes(y=m,x=reorder(label,m))) +
    xlab("")  +
  ylab("Effect on Proportion Infected") +
  geom_pointrange(aes(ymin=ll,ymax=hh,colour=model),alpha=0.8,position=position_dodge(width=0.5),size=.5) +
  ggtitle("Time-Varying Effect on\nProportion Infected") +
  theme_minimal() +
    scale_color_brewer(type="qual") +
    geom_hline(yintercept=0,linetype=2) +
  theme(panel.grid=element_blank(),
        plot.title = element_text(size=10,hjust=0.5),
        legend.position = "top") +
  guides(color=guide_legend(title="")) +
  coord_flip()

p1 + p2 + plot_annotation(tag_levels="A")

ggsave("effect_compare.png")

```

```{r comptests,fig.cap="Comparison of Standardized Smoking Rates and COVID-19 Tests per 10,000 Residents by State"}

check_tests <- group_by(combined,state,Smoking) %>% summarize(mean_tests=(mean(tests,na.rm=T)/state_pop[1])*10000)

check_tests %>% 
  ggplot(aes(y=Smoking,x=mean_tests)) +
    stat_smooth(method="lm",alpha=0.1) +
  geom_text_repel(aes(label=state),alpha=0.8,segment.size=0) +
  theme_minimal() +
  theme(panel.grid = element_blank()) +
  ylab("Smoking Rates (Standard Deviation)") +
  xlab("Tests per 10,000 state residents")




```


# Conclusion

This model was written to permit the identification of suppression measures targeted at the spread of COVID-19. It is not intended to be a replacement or alternative to the disease forecasting literature, especially as this model relies on SIR/SEIR estimates for full identification. If anything, this modeling exercise shows why SIR/SEIR models are so important: without them it is literally impossible to know the total number of infected people on a given day. This model's simplicity and ability to use empirical data are its main features, and the hope is that it can be used and extended by researchers looking at government policies and other tertiary factors on the spread of the disease. At the very least, the model provides realistic uncertainty intervals taking into account very real biases in the observed data.

To fit the model, it is necessary to have at least an estimate of how many tests have been conducted. The  [CoronaNet](https://lumesserschmidt.github.io/CoronaNet/) project is currently working to obtain testing data, in addition to information about government policy responses to COVID-19, in an effort to better understand the role and success of variation in country policy responses to date. 

# Bibliography


